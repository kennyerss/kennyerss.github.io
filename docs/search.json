[
  {
    "objectID": "posts/gradient/index.html",
    "href": "posts/gradient/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: Gradient Descent\n\n\nTo implement gradient descent, I created a gradient_empirical method which would calculate the gradient of the empirical risk formula. This method uses several other methods: predict, and loss_deriv. The predict method is simply a way of generating our predicted \\(y_{hat}\\) by dotting the padded feature matrix \\(X\\) with the current weight vector \\(w\\). The loss_deriv method returns the derivative of our loss function by taking our predicted \\(y_{hat}\\) and computing it on the \\(sigmoid\\) function subtracted by the true label \\(y\\) vector.\nNext, in my fit method, I used gradient_empirical on the padded feature matrix \\(X\\) with the true label vector \\(y\\) for each iteration. Using the result of my gradient empirical method, I can update the next weight vector \\(w^{k+1}\\) to be equal to \\(w^{k} - \\alpha * gradient\\). These updates continue until either the max_epochs have been reached or the loss of \\(w^{k}\\) and \\(w^{k+1}\\) no longer shows any computational difference.\n\n\n\nTo implement stochastic gradient descent with momentum, I implemented a new fit method called fit_stochastic. Inside this method, I create an optional parameter called momentum which when set to \\(True\\), initializes \\(\\beta\\) to \\(0.8\\), otherwise, sets \\(\\beta\\) to \\(0\\) which is just the regular stochastic gradient descent. Stochastic gradient is implemented by grabbing a subset of our data to compute the gradient onto with a parameter batch_size to determine the size of the subset. For every iteration of an epoch, we compute the gradient for that batch of data and update \\(w\\) by computing \\(w^{k} - \\alpha(gradient) + \\beta(w^{k} - w^{k-1})\\). I did this by storing the previous and current \\(w\\) in separate variables, initial_w and curr_w and assign initial_w = curr_w after each update of the current \\(w\\) vector. At the end of an epoch, I compute the loss and append to it to the model’s loss_history."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-1",
    "href": "posts/gradient/index.html#experiment-1",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 1",
    "text": "Experiment 1\nFor our \\(\\textbf{first experiment}\\), we will demonstrate for when the learning rate is too large, that the gradient descent method will not converge to a minimizer. Below, I’ve generated data that is at least 10 feature dimensions for this experiment.\n\nfrom gradient import LogisticRegression \nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nnp.random.seed(901)\n\n# Generate a non-linearly separable data\np_features = 11\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n# Testing the gradient descent with learning rate = 88\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\n\nLR.fit(X, y, 88, 1000)\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\n# Testing the gradient descent with learning rate = 89\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\n\nLR.fit(X, y, 89, 1000)\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nSetting the gradient descent learning rate to be 88 vs 89 to be fitted for 1000 epochs for a datatset with at least 10 features illustrated that there is a threshold for when a minimized loss is found. A possible explanation to this phenomenon could be that a large learning rate overshoots the model to miss the minimizer. Thus, the model would continue going down the direction of the gradient and may oscillate trying to find the minimizer."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-2",
    "href": "posts/gradient/index.html#experiment-2",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 2",
    "text": "Experiment 2\nFor our \\(\\textbf{ second experiment}\\), we will demonstrate the speed of convergence for different choices of batch size for the stochastic gradient algorithm.\n\n# Experimenting stochastic gradient models with different batch sizes\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 5\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 20, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 20\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 50,\n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 50\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nShown above displays the loss evolution of three stochastic gradient models of batch sized 5, 20, and 50. By observation, we can see that as the batch size increases, the model requires more epochs to converge to some minimizer solution. In this particular example, the model with batch size 5 converged the fastest, followed the model with batch size 20, and lastly, the model with batch size 50."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-3",
    "href": "posts/gradient/index.html#experiment-3",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 3",
    "text": "Experiment 3\nFor our \\(\\textbf{third experiment}\\), we will demonstrate the case in which the use of momentum significantly speeds up convergence.\n\n# Experimenting stochastic gradient models with and without momentum\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient without momentum\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = True, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with momentum\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nShown above displays the loss evolution of two stochastic models: one with momentum and one without momentum. By observation, we can conclude that the stochastic gradient model with momentum significantly speeds up convergence to some minimizer solution in comparison to the model without momentum."
  },
  {
    "objectID": "posts/unsupervised/index.html",
    "href": "posts/unsupervised/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "Singular value decomposition (SVD) is another form of dimensionality reduction and matrix factorization similar to principal component analysis (PCA) as we’ve explored in previous lectures.\nRecall that the SVD of a matrix \\(A\\) is:\n\\[A = UDV^{T}\\]\nwhere \\(U\\in\\mathbb{R^{mxm}}\\) and \\(V\\in\\mathbb{R^{nxn}}\\) are orthogonal matrices. The singular values of \\(D\\in\\mathbb{R^{mxn}}\\) along its diagonals overall convey some measure of how big \\(A\\) is. By manipulating the singular values of \\(D\\), we can reconstruct the matrix \\(A\\) from \\(U\\), \\(V\\), and \\(D\\).\nUtilizing SVD allows us to make approximations using a smaller representation of the original matrix; in turn, we can use this approximation in applications with images through image compression!\nThus, in this blog post, we’ll make use of SVD to compress images and analyze the effect of varying singular values of \\(k\\) in assessing efficient storage use for large images on computers with minimal storage.\nFor my RGB image, I’ve chosen a scene from one of my favorite animes, One Piece, with Luffy as the main character shown below.\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://i.pinimg.com/550x/23/e4/e7/23e4e7aa8e7a9e2dbc75fece9d77fc99.jpg\"\n\nimg = read_image(url)\n\nFirst, I’ll read my image using the read_image function which takes in an image url and converts the image as an np array. In doing so, we can represent the image in its RGB dimensions and convert the image to a greyscale image for SVD image compression.\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original Luffy\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale Luffy\")\n\nplt.show()\n\n\n\n\nNow that we’ve converted our image of Luffy to a greyscaled version, we can do compute its SVD to generate a reconstructed image with less storage!\n\n\nHere is the source code for my implementation of SVD: SVD\nIn my source code, I implemented an SVD class containing two main methods: svd_reconstruct and svd_experiment.\nFirstly, I can instantiate an SVD class two arguments to store as its attributes — the image to reconstruct and the number of k components the user wants to use to reconstruct the image. This is defined in the __init__ method of the class.\ndef __init__(self, img, k):\n    self.img = img\n    self.k = k\nNext, I implemented a svd_reconstruct method that again takes in the image to reconstruct, img, and the k components to use. This method returns both the reconstructed image and the total storage needed for the reconstructed image. Shown below is my implementation of the method:\ndef svd_reconstruct(self, img, k):\n    '''\n    Input: Image to reconstruct and number of k singular values to use\n    Output: Reconstructs image from its singular value decomposition and storage amount\n    '''\n    # From lecture notes\n    U, sigma, V = np.linalg.svd(img)\n    \n    # Create the D matrix in the SVD\n    D = np.zeros_like(img,dtype=float) # matrix of zeros of same shape as img\n    # Singular values on the main diagonal\n    D[:min(img.shape),:min(img.shape)] = np.diag(sigma)      \n    \n    # Approximate using the first k columns of U, D, and V\n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    # Reconstruct our image \n    img_ = U_ @ D_ @ V_\n\n    # Get the dimensions of our img\n    m, n = img_.shape\n\n    # Calculate the number of pixels to store for reconstructed image\n    storage = ((k*m) + k + (k*n)) / (m*n) * 100\n\n    return img_, round(storage, 1)\n    \nMost of the code in the svd_reconstruct method were taken from the assignment blog post — specifically reconstructing the image by generating the matrices U, D, and V. In sequential order, I took the first \\(k\\) columns of \\(U\\), the top \\(k\\) singular values of \\(D\\), and the first \\(k\\) rows of \\(V\\). I then computed a matrix multiplication using all three matrices to reconstruct our image img_. Next, I’ll talk about how I calculated the storage needed to represent each reconstructed image.\n\n\nTo calculate the storage needed for a reconstructed image using some \\(k\\) components, I used the information about how each matrix \\(U\\), \\(D\\), and \\(V\\) are “indexed” from the \\(k\\) singular values.\nRecall that for the original image, it takes \\(mn\\) total pixels to represent the image, where \\(m\\) and \\(n\\) are the dimensions of the image (\\(m\\) rows and \\(n\\) columns). For the matrix \\(U\\), taking its first \\(k\\) columns will be represented by \\(km\\) total pixels; similarly, taking the first \\(k\\) rows of the matrix \\(V\\) will be represented by \\(kn\\) pixels. To find out the total pixels needed to represent the matrix \\(D\\), I recognized that the matrix \\(D\\) is a diagonal, square matrix — thus, we can represent and store the matrix using only \\(k\\) pixels even though its dimensions are \\(k\\) by \\(k\\). Lastly, I divided the total pixels of the reconstructed image by \\(mn\\) pixels, followed by a multiplication of \\(100\\) to get its percentage of storage; I ended up rounding the storage by \\(1\\) decimal point to keep the storage amount interpretable.\nThis implementation is described in the code below:\nstorage = ((k*m) + k + (k*n)) / (m*n) * 100\n\n\n\nShown below is a demonstration of using the svd_reconstruct method to reconstruct an image of Luffy.\n\nfrom svd import SVD\nk = 5\nSVD_ = SVD(grey_img, k)\n\n# From lecture notes \ndef compare_images(A, A_):\n\n    fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n\n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n    \nimg_, storage = SVD_.svd_reconstruct(grey_img, k) # Call function to reconstruct image\n\ncompare_images(grey_img, img_)\n\n\n\n\nFor this demonstration, I’ve chosen a \\(k\\) value of \\(5\\). Here, I’ve plotted both the original and reconstructed image using the compare_images function. Though we’ve successfully reconstructed our original image, it is not indistinguishable by eye that the image is indeed Luffy.\nIn the next section, we’ll experiment with different \\(k\\) values using the svd_experiment method. Additionally, we’ll observe where the \\(k\\) threshold lies in determining a recognizable reconstructed image of Luffy.\n\n\n\nThe other SVD class method I’ve chosen to implement is svd_experiment as shown below.\ndef svd_experiment(self):\n'''\nOutput: Plots out varied k singular values and its reconstructed images using SVD\n'''\n# Initialize how many rows and columns we want the subplot axes to have\nplt_row = 3\nplt_col = 3\n\nfig, axarr = plt.subplots(plt_row, plt_col, figsize = (12,6))\n\n# Plotting each new reconstructed image onto subplot\nfor i in range(plt_row):\n    for j in range(plt_col):\n        img_, storage = self.svd_reconstruct(self.img, self.k)\n\n        axarr[i, j].imshow(img_, cmap = \"Greys\")\n        axarr[i, j].axis(\"off\")\n        axarr[i, j].set(title = f\"{self.k} components, % storage = {storage}\")\n\n        # Update k value\n        self.k += 5\n\n# Adjust spacing for each subplot\nplt.tight_layout()\nplt.show()\nThis method uses the attributes img and k initially instantiated when creating an object of the SVD class. For this experimentation, I’ve chosen to use the same \\(k\\) value of \\(5\\) as with my demonstration of svd_reconstruct. I fixed the dimensions of the axes of my subplots to be \\(3\\) by \\(3\\), so that I’ll have at least \\(9\\) reconstructed images of Luffy of increasing \\(k\\) components and storage amount. Additionally, I chose to increase value of \\(k\\) by \\(5\\) after each iteration of my nested for loop.\n\n# Perform experiment\nSVD_ = SVD(grey_img, k = 5)\nSVD_.svd_experiment()\n\n\n\n\nFrom my experiment, I plotted several reconstructed images of Luffy up to \\(45\\) components starting at \\(5\\) components. By observation, the reconstructed image becomes indistinguishable to the original greyscaled image around \\(30\\) components. For the reconstructed image of \\(30\\) components, we only needed to store around \\(14.0\\)% of the original storage amount. Hence, we’ve successfully compressed our original image to reduce the storage amount by \\(86.0\\)%!"
  },
  {
    "objectID": "posts/unsupervised/index.html#laplacian-spectral-clustering-implementation",
    "href": "posts/unsupervised/index.html#laplacian-spectral-clustering-implementation",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Laplacian Spectral Clustering Implementation",
    "text": "Laplacian Spectral Clustering Implementation\nTo implement the Laplacian spectral clustering, recall in the lecture notes that the equation for the normalized random walk Laplacian is:\n\\[ L = D^{-1}[D-A] \\]\nHere, the matrix \\(D\\) is the degree matrix obtained from the adjacency matrix \\(A\\).\n\\[D = [\\sum_{j=1}^{n} A_{i,j}]\\]\nWe then use the normalized Laplacian matrix \\(L\\) to choose some estimate \\(z\\) to minimize the normcut that will split the karate club graph into binary labels corresponding to each node’s club. From the lecture notes, we can find this estimated \\(z\\) as the eigenvector of the normalized Laplacian matrix with the second-smallest eigenvalue.\nIn my implementation of the spectral_clustering function, I first extracted and symmetrized the adjacency matrix, \\(A\\), from my karate club graph \\(G\\). I then used this adjacency matrix to generate the degree matrix \\(D\\) by taking the sum of \\(A\\)’s rows as its diagonal entries.\nThen, we can find the inverse of \\(D\\) using the numpy operationnp.linalg.inv() and calculate the normalized Laplacian matrix \\(L\\) by doing a matrix multiplication between \\(D^{-1}\\) and \\(D - A\\).\nI then found the eigenvalues and eigenvectors of \\(L\\) using the numpy operation np.linalg.eig(). Since we want to choose the eigenvector corresponding to the second-smallest eigenvalue of \\(L\\), I first sorted the eigenvalues by sorted indices in ascending order using the numpy operation np.argsort. Rather than sorting the eigenvalues by its values, I sorted them by indices to easily extract where the index of second-smallest eigenvalue will be.\nLastly, I indexed the eigenvectors of \\(L\\) up to the second-smallest index of the sorted indices in sorted_eig, which would just be at the \\(1\\)th index — finally, we’ve found our estimated cluster vector \\(z\\)!\nWe can retrieve the final set of cluster labels by transforming \\(z\\) as a vector of binary labels. This is done through the line of code: 1*(z_ > 0) — which is just an implementation of the indicator function!\n\ndef spectral_clustering(G):\n    '''\n    Input: Graph G\n    Output: Vector of binary labels to split graph\n    '''\n    # Extract adjacency matrix from graph G\n    A = nx.adjacency_matrix(G).toarray()\n    # Symmetrize the matrix \n    A = A + A.T\n    A[A > 1] = 1\n    \n    # D is the degree matrix obtained from the adjacency matrix\n    D = np.diag(A.sum(axis=1))\n\n    # Generate the normalized Laplacian matrix\n    L = np.linalg.inv(D) @ (D-A)\n    \n    # Using np.linalg.eig to return eigenvalues and eigenvectors in ascending order\n    eig_val, eig_vec = np.linalg.eig(L)\n    \n    # Sort eigenvalues in order to find index of the second smallest eigenvalue\n    sorted_eig = np.argsort(eig_val)\n    \n    z_ = eig_vec[:, sorted_eig[1]] # Obtain the eigenvector with second smallest eigenvalue of L\n    z = 1*(z_ > 0) # Transform into binary labels\n    return z\n\n\nPlotting the Spectral Cluster\nAfter implementing the Laplacian spectral clustering algorithm, we can now plot the set of spectral labels to determine how well its classified each node of the karate club graph.\nRecall that for the binary cluster labels, \\(0\\) represents the Mr. Hi club, and a \\(1\\) would represent the Officer club.\n\n# Generate our cluster labels\n# 0: \"Mr. Hi\", 1: \"Officer\"\ncluster_labels = spectral_clustering(G)\n\n\n# Plotting each graph side by side\nfig, axarr = plt.subplots(1, 2, figsize = (13,6))\n\n# Original graph\nnx.draw_networkx(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\",\n        ax = axarr[0]) \n\n# Spectral graph with random walk normalized Laplacian\nnx.draw_networkx(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if i == 1 else \"steelblue\" for i in cluster_labels],\n        edgecolors = \"black\",\n        ax = axarr[1]) \n\n# Set graph titles \naxarr[0].set(title = \"original clustering\")\naxarr[1].set(title = \"random walk normalized Laplacian spectral clustering\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nAfter plotting both the original clustering and the random walk normalized Laplacian spectral clustering of our graph \\(G\\), we observed that our spectral clustering does significantly well in dividing the graph into similar clusters as found by the original clustering. Only two nodes out of 34, the \\(8th\\) and \\(2nd\\) nodes, were misclassified as part of the Officer club by the spectral clustering algorithm. Thus, the algorithm’s misclassification rate in divding the graph into each respective club is only about \\(6\\)% — pretty good!\nSince the two nodes are right in the middle of the graph, it would be reasonable that our algorithm is unable to correctly classify the two nodes’ clubs."
  },
  {
    "objectID": "posts/timnit-gebru/index.html",
    "href": "posts/timnit-gebru/index.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "On Tuesday, April 24th, Dr. Timnit Gebru will give a talk on the “Eugenics and the Promise of Utopia through Artificial Intelligence” at Middlebury College. Dr. Gebru, a renowned computer scientist who specializes in data mining and algorithmic bias, has long advised for the ethical uses of AI. Dr. Gebru is also a founder of DAIR (Distributed Artificial Intelligence Research insitute), an independent AI research institute to counter the pervasive influences of large tech corporations and ther injustices. In her short tenure in Google in 2018, she co-led a team focused on ethical artificial intelligence. During this time, Dr. Gebru warned of the dangers of a large language model (being the basis of the Google search engine) through her findings in this paper. However, she faced pushback from higher Google executives to withdraw her paper and remove her and other Google team members’ names from the list of co-authors. What spurred from this event was a sequence of back-and-forth arguments with Dr. Gebru paving the way for ethical uses of AI — particularly in her and Joy Buolamwini‘s groundbreaking study of Gender Shades which criticized facial recognition technologies’ flaws in its inability to recognize Black women. Needless to say, the 2020 protests during the Black Lives Matter movement further emphasized the harms and dangers of facial recognition technology against vulnerable communities of color.\nDr. Gebru’s voice in the ethics of AI poses a need to break down the systemic structures that allow AI to exploit communities of color. How long must the development of AI go unchecked as linguistic boundaries regarding the transparency of AI technology blur the rights and wrongs?"
  },
  {
    "objectID": "posts/timnit-gebru/index.html#tutorial-on-fairness-accountability-transparency-and-ethics-fate-in-computer-vision",
    "href": "posts/timnit-gebru/index.html#tutorial-on-fairness-accountability-transparency-and-ethics-fate-in-computer-vision",
    "title": "Learning from Timnit Gebru",
    "section": "Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision",
    "text": "Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision\nIn her talk in the 2020 conference on Computer Vision and Pattern Recognition, Dr. Gebru brings up a quote by Mimi Onuoha to emphasize that it is “imperative to remember on both sides we have human beings” within data sets involving people as subjects or objects. I found this quote to be effective as later on, Dr. Gebru brings up an anecdote given by Seeta Pena Gangadharan, as they reflect on the abstraction of human beings into mathematical equations; this anecdote was important in separating human subjects rather than a means of gathering statistics and mathematical computations. As researchers abstract the reality that people live in into simple data, they fail to recognize the importance of acknowledging the social and power structures that influence and empower algorithmic biases. Dr. Gebru provides several studies such as the Gender Shades project, a wedding classification, and gender classification focused on gender norms.\nAdditionally, the datasets on which models are created may be biased which poses important questions about class imbalance. While there is an inherent need to diversify datasets, there are fundamental questions to how data should be ethically obtained. Dr. Gebru observes that companies have crossed the boundaries of data collection which infringes people’s civil liberties online data being retrieved and used for AI models (e.g. facial recognition) without their consent. Surveillance is a big part of the push to develop facial recognition technologies, and the idea of policing the identities of citizens is consistent with the idea of supremacy and order in society. Thus, federal regulations must be up to date to check the powers of large companies on infringing the consent and privacy of users. Additionally, it’s important that the design of AI models are constructed inclusively with collaboration of communities of color.\nLasty, Dr. Gebru talks about the ethical implications of computer vision as its used today. The continuous receipts of corporations to publish flawed algorithms that harm the historically marginalized emphasizes the need to regulate these algorithms to be more ethically aligned. It’s important to also note that the lack of procedures to test algorithms and systems are not exclusive to AI or tech, and that these exclusions can also be traced to the history of medical research. The development of computer vision benefits the large corporations who have the manpower, resources, and finances to support the systems of weaponry, policing, and oppression by the military and government agencies.\ntldr; Computer vision as it’s used today transforms the intent of the designer to perpetuate systems of oppression we live in today, furthering the harms of those historically marginalized and excluded."
  },
  {
    "objectID": "posts/timnit-gebru/index.html#questions",
    "href": "posts/timnit-gebru/index.html#questions",
    "title": "Learning from Timnit Gebru",
    "section": "Questions",
    "text": "Questions\nHere is my question I would like to ask Dr. Gebru:\nWhat role should the designer play in the way AI processes are created, maintained, and commercialized? This question is in the context of design — specifically the design of AI and its role in perpetuating systems that enhance racial and gender biases."
  },
  {
    "objectID": "posts/timnit-gebru/index.html#part-2-dr.-timnit-gebrus-talk-at-middlebury-college",
    "href": "posts/timnit-gebru/index.html#part-2-dr.-timnit-gebrus-talk-at-middlebury-college",
    "title": "Learning from Timnit Gebru",
    "section": "Part 2: Dr. Timnit Gebru’s Talk at Middlebury College",
    "text": "Part 2: Dr. Timnit Gebru’s Talk at Middlebury College\n\nIn-Class Virtual Talk\nAs a precursor to Dr. Gebru’s virtual talk at Middlebury College, our class had a chance to talk to her via questions that my peers had written up. Some important takeways from the brief introduction to Dr. Gebru were that:\n\nThere are no incentives to large corporations publishing flawed algorithms other than maximizing profit.\nThe development of larger and larger language models pushes all company resources towards the direction of the next “big” thing rather than smaller alternatives.\nThe military is the BIGGEST investor of AI. So, when the foundation of what you’re building is driven by military applications, the main motivation then become a question of how can we better weaponize + strategize our military?\nThe rapid development of AI is grounded on worker exploitation to do the ‘classifiying’ on data sets with zero compensation.\nHuman agency to participate in online activity is taken away from us — either you participate, or you get flagged as ‘high-risk’ for not participating.\n\nListed above are just some of the takeaways I found important in grounding what the virtual talk would feel like later that night. I found it most shocking, yet so understanding, that the military — out of all institutions – would be the biggest financial supporter for the development of AI technologies. It just made sense that the U.S., whose federal spending on national defense accounts one-sixth of the total national spending, is pushing billions of dollars to develop technologies that are still yet to be heavily regulated and maximize AI applications to warfare, surveillance, and autonomous vehicles.\n\n\nHillcrest Virtual Talk\nDr. Gebru’s virtual talk addresses the main question about the development of Artificial General Intelligence (AGI): Who decides what utopia exists, and for whom? AGI as a term is not well-defined, and generalizing what AGI could do for humanity, it seems as the development of AGI is equivalent to building some sort of God. She argues that the utopian rhetoric surrounding the advancement of AGI, as suggested by key figures in the tech industry such as Sam Altman, Elon Musk, etc., is grounded on dangerous eugenic-based ideologies referencing to a second-wave of eugenics, the TESCREAL bundle. As a result, the realities of what AGI could be are exaggerated to mimic that of a sci-fi movie and distracts from the actual realities of worker exploitation, fraudulent datasets, and power vacuums.\nDr. Gebru first introduces the talk with the history and myths of eugenics. Before her talk, if I heard the word ‘eugenics’, I would think of the Holocaust with the Nazis and forced sterilizations of the disabled, Blacks, and other vulnerable communities. However, eugenics is not equivalent to the Nazis, nor did it certainly go away after WWII. She explains that in the first wave of eugenics, powerful statisticians, physicians, and scientists paved the way in defining eugenics as a way of improving the “human stock.” Eugenics can be classified into two categories: positive and negative. Negative eugenics would be the idea of getting rid of undesirable traits (e.g. the Nazis in WWII). However, the rhetoric of eugenics reinvented itself to form the second-wave of eugenics by providing positive ways to “improve the human stock.” Rather than sterilizing groups of people, scientific advancements posed the option to provide people the ability to “design” their own children. I thought about the movie Gattaca, which is about a dystopian future where genomic interventions had been 100% successful in “designing” the genetic makeup of children and exposes the racial discriminations of the “legacy-human” and “post-human”, exploring the ideology of genetic determinism led by Francis Galton. Dr. Gebru then elaborates on the reinvention of eugenics through the TESCREAL bundle (Transhumanism, Extropianism, Singularitarianism, Cosmism, Rationalism, Effective Altruism, and Longtermism). While I won’t define each category of the bundle, it’s important to note that the TESCREAL bundle all have a direct line going back to the first-wave eugenics with motivations in “transcending” humanity. Additionally, they all want to “radically transform and modify human organism through AI.” By “transcending” humanity, AGI is able to create a utopian paradise where AGI is able to do anything for everyone — again, a God.\nHowever, there are major ethical and practical issues with building an “AGI paradise”, as Dr. Gebru advises that:\n\nAn AGI paradise in the form of very very large-scale language models, centralizes power and monopolizes all the services you can do through this AGI utopia. This centralizing of power risks the further separation of classes as the rich can have real doctors, whereas the poor can go to this AGI paradise to solve their problems. However, this is not the case as even an AGI utopia will not be able to fix, for instance, climate crisis refugees who are forcibly moved out of their homes due to uncontrollable circumstances with the burning of fossil fuels and greenhouse gases. An AGI utopia simplifies all problems that anyone can have as something that is fixable, ignoring the historically identified structures of oppressions.\nThe labor and computing costs for developing an AGI paradise will exceed what is humanly possible. Horrendous worker exploitation will continue, and the need for larger data sets vacuum fundings from smaller institutions doing real work to improve their local communities. Furthermore, an AGI paradise is not a well-scoped system that has narrow and well-defined use cases.\n“Safe AGI” is unsustainable and is a manipulation of rhetoric to develop AGI that is aligned with humanity’s values. Additionally, a universal design approach being a one model fits all simplifies what different users actually need.\nThe unregulated field of AGI conducts a revisioning of technology. What is considered non-normal will now become normal. Also, there is an inherent “truth” in the tech industry that whatever is being spewed out will and should work, however no proof is ever needed to test XYZ.\n\nDr. Gebru’s talk left me pondering throughout the rest of the night with how powerful language and rhetoric is in shifting public understanding of AGI and its potential uses. While it’s heavily controversial to consider that the development of AGI is in line with eugenics, I agree with her argument to some extent. I think the delays in regulating AGI development and accountability for Big Tech is long overdue, and it’s quite surprising how long corporations have evaded the responsibilities and punishments for actions simply unacceptable such as stealing data from small artists. I think one thing that people should learn from Dr. Gebru’s discussion is that there is no real future in suggesting that AGI could transcend humanity and fix every problem out there, and we should be able to re-claim agency about what is considered “good” for humanity’s advancement; as Dr. Gebru quotes, “It is not an equitable place for corporations to choose what type of problems they want their language models to tackle.”"
  },
  {
    "objectID": "posts/timnit-gebru/index.html#reflection-of-dr.-gebrus-work",
    "href": "posts/timnit-gebru/index.html#reflection-of-dr.-gebrus-work",
    "title": "Learning from Timnit Gebru",
    "section": "Reflection of Dr. Gebru’s Work",
    "text": "Reflection of Dr. Gebru’s Work\nFrom this experience, I grew to appreciate Dr. Gebru’s work even more beyond what I regularly believed engineers in Computer Science do with a 9-5 job to build software without a care for its applications. Learning from Dr. Gebru has taught me the importance of critically thinking about who is benefitting from software and why they would want to pursue its developments. Diving into the intersectionality of ethics and technology has found itself to be a frustrating experience personally, because there is just so much work to do. However, this has definitely peaked my interest in being more intentional with large language models and informing myself of the ethical implications of AGI."
  },
  {
    "objectID": "posts/linear-reg/index.html",
    "href": "posts/linear-reg/index.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Here is the link to my source code: Linear Regression\n\n\nFor this blog post, I’ve implemented least-squares linear regression in two ways: using the analytical method and an efficient gradient descent.\n\n\nThe analytical method is derived from solving the equation: \\[0 = X^{T}(X \\hat{w} - y)\\]\nwhere \\(X\\) is our paddded feature matrix. By setting the gradient equal to \\(0\\), we can solve for \\(\\hat{w}\\) to get an explicit solution for the optimized weight vector. Solving this equation requires \\(X\\) to be an invertible matrix such that it has at least many rows as columns. Thus, our final solution for \\(\\hat{w}\\) is\n\\[\\hat{w} = (X^{T}X)^{-1} X^{T}y\\]\nIn my fit_analytic method, I utilized numpy’s linalg transpose and inverse methods alongside orderly matrix multiplications to calculate the optimized weight vector \\(\\hat{w}\\).\n\n\n\nTo implement an efficient gradient descent for least-squares linear regression, instead of computing the original gradient equation at each iteration of an epoch:\n\\[\\nabla{L(w)} = X^{T}(Xw - y)\\]\nI calculated once \\(P = X^{T}X\\) and \\(q = X^{T}y\\) to reduce the time complexity of the matrix mutliplication of \\(X^{T}X\\) being \\(O(np^2)\\) and \\(X^{T}y\\) being \\(O(np)\\). Thus, reducing the gradient equation to be:\n\\[\\nabla{L(w)} = Pw - q\\]\nreduces the time complexity of calculating the gradient to be \\(O(p^2)\\) steps which is significantly faster! In my fit_gradient method, I first initialized some random weight vector of \\(p\\) shape as my padded \\(X\\) feature matrix. Then, I computed \\(P\\) and \\(q\\) which I used inside my for-loop to update my weight vector \\(self.w\\) with the gradient. At each epoch, I calculated the score of the current weight vector and appended the current score to score_history."
  },
  {
    "objectID": "posts/linear-reg/index.html#demonstration",
    "href": "posts/linear-reg/index.html#demonstration",
    "title": "Implementing Linear Regression",
    "section": "Demonstration",
    "text": "Demonstration\nShown below, I’ve generated a set of data using the LR_data method in my LinearRegression class. Then, I fit the data using both analytic and gradient descent methods and should expect a similar optimized weight vector \\(w\\).\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom linear import LinearRegression \nimport numpy as np \nfrom matplotlib import pyplot as plt\n\nLR = LinearRegression()\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nnp.random.seed(1)\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\nLR.fit_analytic(X_train, y_train) \nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.4765\nValidation score = 0.4931\n\n\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = int(1e2))\n\nThen, I can plot the score_history of the gradient descent to see how the score evolved until the max iterations. By observation, the score evolved monotonically since we’re not using stochastic gradient.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(title = \"Evolution of Training Score\", xlabel = \"Iteration\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-reg/index.html#experiment-1-increasing-p_features-with-constant-n_train",
    "href": "posts/linear-reg/index.html#experiment-1-increasing-p_features-with-constant-n_train",
    "title": "Implementing Linear Regression",
    "section": "Experiment 1: Increasing p_features with constant n_train",
    "text": "Experiment 1: Increasing p_features with constant n_train\nFor this first experiment, I’ve chosen to increase the number of p_features to \\(10\\), \\(50\\), and then later choose the number of p_features to be \\(n-1\\).\n\nnp.random.seed(4)\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR1 = LinearRegression()\n\nLR1.fit_analytic(X_train, y_train) \n\nLR1_train_score = LR1.score(X_train, y_train).round(4)\nLR1_validation_score = LR1.score(X_val, y_val).round(4)\n\n\nnp.random.seed(2)\n\np_features = 50\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR2 = LinearRegression()\n\nLR2.fit_analytic(X_train, y_train) \n\nLR2.fit_analytic(X_train, y_train) \n\nLR2_train_score = LR2.score(X_train, y_train).round(4)\nLR2_validation_score = LR2.score(X_val, y_val).round(4)\n\n\nnp.random.seed(3)\n\np_features = n_train - 1 \nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR3 = LinearRegression()\n\nLR3.fit_analytic(X_train, y_train) \n\nLR3_train_score = LR3.score(X_train, y_train).round(4)\nLR3_validation_score = LR3.score(X_val, y_val).round(4)\n\nWe can visualize the training and validation scores to visibly observe the differences of each experiment as we increase the number of p_features.\n\n# Code from https://www.geeksforgeeks.org/bar-plot-in-matplotlib/\nbar_width = 0.2\n\ntraining_scores = [LR1_train_score, LR2_train_score, LR3_train_score]\nvalidation_scores = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(training_scores))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br1, training_scores, width = bar_width,\n        edgecolor ='grey', label ='training score')\nplt.bar(br2, validation_scores, width = bar_width,\n        edgecolor ='grey', label ='validation score')\n\nplt.title('Number of p_features vs score with constant n_train')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(training_scores))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\nAs shown on the graph above, we can see that as the number of p_features increases up to n_train - 1, the fitted model’s training_score also increases. However, the model’s validation score decreases. This conclusion is related to the model being overfit due to the significant difference of the training and validation score between the model with n_train - 1 p_features. This means that we’ve trained the model exactly to some random training data given, however, when validated on the true labels, the calculated optimized weight vector \\(w\\) will be highly inaccurate in comparison to the labels."
  },
  {
    "objectID": "posts/linear-reg/index.html#experiment-2-lasso-regularization",
    "href": "posts/linear-reg/index.html#experiment-2-lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "Experiment 2: LASSO Regularization",
    "text": "Experiment 2: LASSO Regularization\nUsing LASSO regularization, we modify the original loss function to add a regularization term:\n\\[L(w) = ||Xw - y||^{2}_{2} + \\alpha||w'||_{1}\\]\nThis extension of the regularization term minimizes the weight vector \\(w\\) as small as it could be and forces the weight vector’s entries to be exactly zero.\n\nVarying Degrees of Alpha\nFor this experiment, we can choose varying degrees of alpha while increasing the number of p_features of our data.\n\nfrom sklearn.linear_model import Lasso\n\n\n# Alpha of 0.001\nL1 = Lasso(alpha = 0.001)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_1 = L1.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_2 = L1.score(X_val, y_val)\n\n\np_features = n_train - 1 \n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_3 = L1.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.001\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L1_validation_1, L1_validation_2, L1_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='standard linear regression')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso regularization')\n\nplt.title('Number of p_features vs Validation Score for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Validation Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\n\n# Alpha of 0.01\nL2 = Lasso(alpha = 0.01)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_1 = L2.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_2 = L2.score(X_val, y_val)\n\n\np_features = n_train - 1\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_3 = L2.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.01\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L2_validation_1, L2_validation_2, L2_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='standard linear regression')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso regularization score')\n\nplt.title('Number of p_features vs Validation Score for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Validation Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\n\n# Alpha of 0.1\nL3 = Lasso(alpha = 0.1)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_1 = L3.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_2 = L3.score(X_val, y_val)\n\n\np_features = n_train - 1\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_3 = L3.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.1\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L3_validation_1, L3_validation_2, L3_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='standard linear regression')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso regularization')\n\nplt.title('Number of p_features vs Validation Scores for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Validation Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\n\n\n\n\n\n# Plotting all three alpha levels\n\nbar_width = 0.2\n\nlasso_validation_1 = [L1_validation_1, L1_validation_2, L1_validation_3]\nlasso_validation_2 = [L2_validation_1, L2_validation_2, L2_validation_3]\nlasso_validation_3 = [L3_validation_1, L3_validation_2, L3_validation_3]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation_1))\nbr2 = [x + bar_width for x in br1]\nbr3 = [x + bar_width for x in br2]\n\n# Make the plot\nplt.bar(br1, lasso_validation_1, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.001')\nplt.bar(br2, lasso_validation_2, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.01')\nplt.bar(br3, lasso_validation_3, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.1')\n\nplt.title('Number of p_features vs Validation Score for Lasso Regularization with Varying Alpha Degrees')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Validation Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\n\n\n\n\nAfter plotting all three degrees of alpha with increasing number of p_features up to n_train - 1, I found that smaller values of alpha (alpha = \\(0.001\\)) will still retain a moderately high validation score despite reaching up to n_train - 1 number of p_features. However, as I increase the degree of alpha to \\(0.01\\) and \\(0.1\\), the difference in validation score between LASSO regularization and standard linear regression becomes significantly different. As I increase the strength of the regularizer, the validation score for LASSO regularization approaches zero, and is no longer accuracte in predicting the true labels.\nIn conclusion, LASSO regularization can improve a model’s validation score with smaller alpha levels in contrast to utilizing standard linear regression even with up to n_train - 1 number of p_features. However, as you increase the strength of the regularization, the validation score decreases significantly, and the model is no longer proficient in its predictions."
  },
  {
    "objectID": "posts/audit/index.html",
    "href": "posts/audit/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "For this analysis, I’ve chosen to pull the 2018 PUMS data from the state of Illinois.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"IL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\n\n# Reducing number of features\npossible_features=['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'RAC1P', 'SEX', 'PINCP']\nacs_data[possible_features]\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      POBP\n      RELP\n      WKHP\n      RAC1P\n      SEX\n      PINCP\n    \n  \n  \n    \n      0\n      86\n      NaN\n      16.0\n      2\n      NaN\n      17\n      16\n      NaN\n      1\n      2\n      22200.0\n    \n    \n      1\n      30\n      1.0\n      18.0\n      5\n      4220.0\n      22\n      17\n      99.0\n      1\n      2\n      15000.0\n    \n    \n      2\n      57\n      NaN\n      19.0\n      5\n      NaN\n      17\n      17\n      NaN\n      2\n      1\n      0.0\n    \n    \n      3\n      69\n      1.0\n      19.0\n      3\n      1551.0\n      20\n      16\n      NaN\n      1\n      1\n      21600.0\n    \n    \n      4\n      18\n      3.0\n      18.0\n      5\n      2350.0\n      6\n      17\n      NaN\n      2\n      1\n      350.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      126451\n      72\n      NaN\n      16.0\n      1\n      NaN\n      17\n      1\n      NaN\n      1\n      1\n      68800.0\n    \n    \n      126452\n      43\n      1.0\n      21.0\n      1\n      710.0\n      210\n      0\n      40.0\n      6\n      1\n      89000.0\n    \n    \n      126453\n      39\n      NaN\n      21.0\n      1\n      NaN\n      210\n      1\n      NaN\n      6\n      2\n      0.0\n    \n    \n      126454\n      7\n      NaN\n      4.0\n      5\n      NaN\n      210\n      2\n      NaN\n      6\n      2\n      NaN\n    \n    \n      126455\n      5\n      NaN\n      1.0\n      5\n      NaN\n      210\n      2\n      NaN\n      6\n      1\n      NaN\n    \n  \n\n126456 rows × 11 columns\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"PINCP\", \"SEX\"]]\n\nBelow is a preprocessing step to filter our data in corresponding to the literature’s filters over the variables AGEP (age), PINCP (total income), WKHP (usual hours worked in the last 12 months), and PWGTP (individual’s weight).\n\n# source code taken from folktables PUMS problem constructions\ndef adult_filter(data):\n    \"\"\"Mimic the filters in place for Adult data.\n    Adult documentation notes: Extraction was done by Barry Becker from\n    the 1994 Census database. A set of reasonably clean records was extracted\n    using the following conditions:\n    ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n    \"\"\"\n    df = data\n    df = df[df['AGEP'] > 16]\n    df = df[df['PINCP'] > 100]\n    df = df[df['WKHP'] > 0]\n    df = df[df['PWGTP'] >= 1]\n    return df\n\n\n# Transform our IncomeProblem to a BasicProblem to minimize features we would like to use\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: 1*(x > 50000),\n    group='SEX',\n    preprocess= adult_filter,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n\n# Split our data into test and train sets \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)"
  },
  {
    "objectID": "posts/audit/index.html#basic-descriptives",
    "href": "posts/audit/index.html#basic-descriptives",
    "title": "Auditing Allocative Bias",
    "section": "Basic Descriptives",
    "text": "Basic Descriptives\n\n# Convert our data into a dataframe for visualization\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\ndf\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      POBP\n      RELP\n      WKHP\n      RAC1P\n      group\n      label\n    \n  \n  \n    \n      0\n      48.0\n      1.0\n      20.0\n      1.0\n      3515.0\n      17.0\n      1.0\n      40.0\n      1.0\n      2\n      0\n    \n    \n      1\n      49.0\n      1.0\n      21.0\n      1.0\n      850.0\n      17.0\n      1.0\n      40.0\n      1.0\n      1\n      1\n    \n    \n      2\n      44.0\n      1.0\n      1.0\n      1.0\n      4251.0\n      303.0\n      12.0\n      40.0\n      1.0\n      1\n      0\n    \n    \n      3\n      46.0\n      1.0\n      23.0\n      1.0\n      440.0\n      17.0\n      0.0\n      40.0\n      1.0\n      2\n      1\n    \n    \n      4\n      36.0\n      1.0\n      16.0\n      1.0\n      5610.0\n      17.0\n      1.0\n      60.0\n      1.0\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53607\n      32.0\n      1.0\n      20.0\n      1.0\n      8740.0\n      17.0\n      0.0\n      48.0\n      1.0\n      1\n      0\n    \n    \n      53608\n      75.0\n      3.0\n      22.0\n      1.0\n      2320.0\n      17.0\n      0.0\n      7.0\n      1.0\n      1\n      0\n    \n    \n      53609\n      53.0\n      1.0\n      16.0\n      3.0\n      4220.0\n      17.0\n      10.0\n      6.0\n      1.0\n      2\n      0\n    \n    \n      53610\n      57.0\n      3.0\n      16.0\n      1.0\n      7000.0\n      17.0\n      1.0\n      50.0\n      1.0\n      1\n      0\n    \n    \n      53611\n      37.0\n      3.0\n      17.0\n      1.0\n      3945.0\n      17.0\n      0.0\n      40.0\n      1.0\n      1\n      1\n    \n  \n\n53612 rows × 11 columns\n\n\n\nIn this dataset, there are 53612 total individuals present.\n\n# Calculate the proportion of individuals who have an income of over $50k\ndf[\"label\"].mean()\n\n0.39455718868909945\n\n\nAdditionally, of the 53612 individuals in the data, only about \\(39.5\\)% of the individuas have an income of over $50k.\n\n# Calculate the total individuals per group\ngroup_prop = df.groupby(\"group\", as_index = False).size()\ngroup_prop.rename(columns = {\"size\" : \"total\"})\n\n\n\n\n\n  \n    \n      \n      group\n      total\n    \n  \n  \n    \n      0\n      1\n      27724\n    \n    \n      1\n      2\n      25888\n    \n  \n\n\n\n\nIn this data, there are a total of 27724 males and 25888 females identifying folks.\n\ngroup_label = df.groupby([\"group\"], as_index = False)[\"label\"].mean()\ngroup_label\n\n\n\n\n\n  \n    \n      \n      group\n      label\n    \n  \n  \n    \n      0\n      1\n      0.478611\n    \n    \n      1\n      2\n      0.304543\n    \n  \n\n\n\n\nIn the male group, about \\(47.9\\)% of male individuals have an income of over 50k. Whereas in the female group, only about \\(30.5\\)% of female individuals have an income of over 50k.\nHere, we can visualize the intersectional relationship between the proportion of individuals in the SEX and RAC1P groups who have an income of over $50k.\n\n# Compute proportion of positive labels by race and sex\nrace_prop = df.groupby([\"RAC1P\", \"group\"], as_index = False)[\"label\"].mean()\n\n\n# Visualize positive labels for both and race\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport seaborn.objects as so\n\nsns.catplot(data=race_prop, x=\"RAC1P\", y=\"label\", hue=\"group\", kind=\"bar\")\n\n# Add labels to title, x and y axis\nplt.title(\"Distribution of Race and Sex with Income Over $50K\")\nplt.xlabel(\"Race Group\")\nplt.ylabel(\"Proportion of Individuals with Income Over $50K\")\n\nplt.show()\n\n\n\n\nShown above is a visualization of the different racial groups studied and their proportion of individuals who have an income of over $50k grouped by sex in Illinois.\nOne major observation from the plot is that the male sex has a larger proportion of individuals who have an income of over 50k than the female sex for all race groups with the exceptions to race groups 4 and 7 — Alaska Natives alone and Native Hawaiian and Other Pacific Islanders, respectively. Alaska Natives alone are also outliers in this plot as both sex groups show zero proportion of all individuals who have an income of over 50k in Illinois; similarly, females in the race group 5 (American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races) show zero proportion of all individuals who have an income of over 50k.\nAll RAC1P groups and other variables used in this analysis are described in the Appendix here."
  },
  {
    "objectID": "posts/audit/index.html#tuning-complexity-of-decisiontree",
    "href": "posts/audit/index.html#tuning-complexity-of-decisiontree",
    "title": "Auditing Allocative Bias",
    "section": "Tuning Complexity of DecisionTree",
    "text": "Tuning Complexity of DecisionTree\nNow that we’ve explored our data, we can train a machine learning model to predict the individuals living in Illinos who have an income of over $50k. Using the DecisionTree model, I can tune the depth using a cross-validation of 10-fold to choose a tree that maximizes the accuracy.\nFor each level complexity of our tree, we can assess its performance and verify which depth we should use for our model.\n\n# From Classifier Lecture\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1)\n\n# Plotting each of the complexity and its performance \nfor d in range(2, 15):\n    T = DecisionTreeClassifier(max_depth = d)\n    m = cross_val_score(T, X_train, y_train, cv = 10).mean()\n    ax.scatter(d, m, color = \"black\")\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", \n              ylabel = \"Performance (score)\", \n              title = \"Performance (score) vs Complexity (depth) of DecisionTreeClassifier\")\n\n\n\n\nFrom the plot, it is observed that a complexity of roughly 11 is the best for the data set which maximizes the performance of the DecisionTreeClassifier.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nWe can now train our model and fit it over the testing set:\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = 11))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=11))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=11))])StandardScalerStandardScaler()DecisionTreeClassifierDecisionTreeClassifier(max_depth=11)"
  },
  {
    "objectID": "posts/audit/index.html#overall-measures",
    "href": "posts/audit/index.html#overall-measures",
    "title": "Auditing Allocative Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\n\nOverall Accuracy\n\n# Generate predictions based off model\ny_hat = model.predict(X_test)\n\nThe overall accuracy in predicting whether an individual has an income of over $50k is:\n\ntotal_acc = (y_hat == y_test).mean()\ntotal_acc\n\n0.7805132796180245\n\n\nThus, our model achieved a \\(78\\)% accuracy in predicting whether a individual has an income of over $50k in Illinois.\n\n\nPositive Predictive Value (PPV)\n\nfrom sklearn.metrics import confusion_matrix\n# Compute the confusion matrix for the model\ncm = confusion_matrix(y_test, y_hat, normalize = \"true\")\n\n# Extract PPV, FPR, and FNR\nppv = cm[0][0]\nfpr = cm[0][1]\nfnr = cm[1][0]\n\ncm\n\narray([[0.80999511, 0.19000489],\n       [0.26570881, 0.73429119]])\n\n\nThe positive predictive value (PPV) can be calculated given the equation:\n\\[PPV = \\frac{TP}{TP + FP}\\]\nFortunately, our confusion matrix does this calculation for us! As shown in the top left corner of the confusion matrix of the overall PPV of our model is about \\(81\\)%.\nHence, of the total population of the dataset, about \\(81\\)% of individuals who reside in Illinois who does have an income of over $50k is true.\n\n\nFalse Negative Rate (FNR) and False Positive Rate (FPR)\nSimilarly, we can extract the overall FNR and FPR of our model without having to compute its respective formulas using the confusion matrix. The FNR (shown in the bottom left corner) is about \\(27\\)%, and the FPR (shown in the top righ corner) is about \\(19\\)%.\nOur model has a FNR of about \\(27\\)% which suggests that the model is classifying individuals with income of over $50k as having an income below the threshold nearly a third of the time.\nFurthermore, our model has a FPR of about \\(19\\)% which suggests that the model is classifying individuals with income below $50k as having an income above the threshold about 19% of the time.\nNow that we’ve audited our overall model, we can check if similar measures are observed for both male and female groups."
  },
  {
    "objectID": "posts/audit/index.html#by-group-measures",
    "href": "posts/audit/index.html#by-group-measures",
    "title": "Auditing Allocative Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\nWe can further our audit analysis by checking the previous measures for each group male and female who makes an income of over $50k.\n\nMale Individuals\nHere is the confusion matrix for male individuals in Illinois who make an income of over $50k:\n\nmcm = confusion_matrix(y_test[group_test == 1], y_hat[group_test == 1], normalize = \"true\")\n\n# Extract PPV, FPR, and FNR\nmppv = mcm[0][0]\nmfpr = mcm[0][1]\nmfnr = mcm[1][0]\n\nmcm\n\narray([[0.77290287, 0.22709713],\n       [0.25985222, 0.74014778]])\n\n\n\nMale Accuracy\n\nmale_acc = (y_hat == y_test)[group_test == 1].mean()\nmale_acc\n\n0.7574214202561118\n\n\nOur model correctly predicts whether a male individual in Illinois has an income of over $50k about 76% of the time.\n\n\nPositive Predictive Value (PPV)\nThe PPV of my model for male individuals is about 77% which suggests that for this population, our model correctly classifies male individuals in Illinois who does have an income of over $50k (true positives) 77% of the time.\n\n\nFalse Negative Rate (FNR) and False Positive Rate (FPR)\nThe FNR and FPR of my model for male individuals is about \\(26\\)% and \\(23\\)%, respectively.\nHence, our model does not classify the true positives (male individuals who have an income of over $50k) 26% of the time; in other words, 26% of male individuals who have an income above the 50k threshold are classified as having an income below the threshold.\nLastly, given the FPR of nearly 23%, our model incorrectly classifies male individuals who have an income below $50k as true positives nearly 23% of the time.\n\n\n\nFemale Individuals\nNow, let’s check the probability measures for female individuals. Shown below is the confusion matrix for female individuals who make an income of over $50k:\n\nfcm = confusion_matrix(y_test[group_test == 2], y_hat[group_test == 2], normalize = \"true\")\n\n# Extract PPV, FPR, and FNR\nfppv = fcm[0][0]\nffpr = fcm[0][1]\nffnr = fcm[1][0]\n\nfcm\n\narray([[0.83947368, 0.16052632],\n       [0.27535497, 0.72464503]])\n\n\n\nFemale Accuracy\nThe accuracy for predicting whether a female individual in Illinois has an income of over $50k is:\n\nfemale_acc = (y_hat == y_test)[group_test == 2].mean()\nfemale_acc\n\n0.8048071034905082\n\n\nOur model correctly predicts whether a female individual has an income of over $50k about 80% of the time.\n\n\nPositive Predictive Value (PPV)\nObserving the confusion matrix for the female group, we can see that the PPV of our model is about \\(84\\)%.\nHence, our model classifies the true positives, female individuals who does have an income of over $50k, as true nearly 84% of the time.\n\n\nFalse Negative Rate (FNR) and False Positive Rate (FPR)\nOur model displays the FNR and FPR for female individuals to be about 27% and 16%, respectively.\nHence, our model does not classify the true positives (female individuals who have an income of over $50k) 27% of the time; in other words, 27% of female individuals who have an income above the 50k threshold are classified as having an income below the threshold.\nLastly, given the FPR of about 16%, our model incorrectly classifies female individuals who have an income below $50k as true positives about 16% of the time.\n\n\nComparison\nAfter auditing for both groups, we observe that both male and female groups have a similar FNR which suggests that the model is incorrectly missing a similar proportion of individuals in Illinois who have an income above $50k (but slightly higher for female individuals).\nHowever, the FPR for the female group is lower than the male group. Thus, our model is generating fewer false positives for female individuals than male individuals; in other words, for female individuals who have an income below $50k, our model will classify them to belong in the negative class (individuals who have an income below 50k) about 84% of the time — while male individuals are only classified to belong in the negative class about 77% of the time.\nShown below is a visualization of the model’s overall measures and of each subgroup’s.\n\nbar_width = 0.2\n\ntotal = [total_acc, ppv, fpr, fnr]\nmale = [male_acc, mppv, mfpr, mfnr]\nfemale = [female_acc, fppv, ffpr, ffnr]\n\n\n# Set position of bar on X axis\nbr1 = np.arange(len(total))\nbr2 = [x + bar_width for x in br1]\nbr3 = [x + bar_width for x in br2]\n\n# Make the plot\nplt.bar(br1, total, width = bar_width,\n        edgecolor ='grey', label ='overall')\nplt.bar(br2, male, width = bar_width,\n        edgecolor ='grey', label ='male')\nplt.bar(br3, female, width = bar_width,\n        edgecolor ='grey', label ='female')\n\nplt.title(\"Distribution of Model's Overall and By-Group Measures\")\n\n# Adding Xticks\nplt.xlabel('Measures', fontsize = 12)\nplt.ylabel('Probability', fontsize = 12)\nplt.xticks([r + 0.2 for r in range(len(total))],\n           ['Accuracy', 'PPV', 'FPR', 'FNR'])\n\nlegend = plt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/audit/index.html#bias-measures",
    "href": "posts/audit/index.html#bias-measures",
    "title": "Auditing Allocative Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\n\nCalibration\nNow, we can assess whether our model is reasonably calibrated by grouping by the average of both group and the true_label over the predicted_label. For this analysis, a calibrated model means that the fraction of individuals who have an income of over $50k is the same across both the male and female groups.\nTo do this, I first created a new data frame containing my group (SEX), true_label (y_test), and my predicted labels (y_hat). Then, I mutated the “group” column to Male (1) and Female (2) for visual readability.\n\n# Create a dataframe containing the true and predicted labels and groups\ncalib_features = [\"group\", \"true_label\", \"predicted_label\"]\ncalib_df = pd.DataFrame({\"group\": group_test, \"true_label\": y_test, \"predicted_label\": y_hat}, columns = calib_features)\n\n# Mutate df group values for readability using .replace()\ngroup_test = calib_df[\"group\"].replace([1, 2], [\"Male\", \"Female\"])\ncalib_df = pd.DataFrame({\"group\": group_test, \"true_label\": y_test, \"predicted_label\": y_hat}, columns = calib_features)\ncalib_df.head()\n\n\n\n\n\n  \n    \n      \n      group\n      true_label\n      predicted_label\n    \n  \n  \n    \n      0\n      Female\n      0\n      0\n    \n    \n      1\n      Male\n      0\n      1\n    \n    \n      2\n      Male\n      0\n      0\n    \n    \n      3\n      Male\n      0\n      1\n    \n    \n      4\n      Female\n      1\n      1\n    \n  \n\n\n\n\nAfter creating the new dataframe, I generated two visualizations: a line plot and bar plot of the mean across the groups’ predicted labels.\n\n# Perform calibration test using .groupby() method\nmeans = calib_df.groupby([\"group\", \"true_label\"])[\"predicted_label\"].mean().reset_index(name = \"mean\")\n\n# Visualizations of \ng = sns.lineplot(data = means, x = \"true_label\", y = \"mean\", hue = \"group\")\n\nplt.title(\"Line Plot of Mean Across Sub-groups Labels\")\nplt.xlabel(\"True Label\")\nplt.ylabel(\"Mean\")\nplt.show()\n\n\n\n\n\np = sns.barplot(data = means, x = \"true_label\", y = \"mean\", hue = \"group\")\n\nplt.title(\"Distribution of Mean Across Group Labels\")\nplt.xlabel(\"True Label\")\nplt.ylabel(\"Mean\")\nplt.show()\n\n\n\n\nFrom the perspective of calibration, the model might suggest that it is biased in the direction of the Male group. Out of those who were predicted to have an income of over $50k, slightly more male individuals were predicted to have an income over the income threshold in comparison to Female individuals.\nAdditionally, the line plot shows that across the ranges of true labels, male individuals maintain a higher statistical mean and proportion who are predicted to make an income of over $50k than female individuals. Thus, both plots can argue that the model is biased in the direction of the male group.\nHowever, since the rates for both groups are sufficiently close, it may also be correct to suggest that the model appears well-calibrated.\n\n\nError Rate Balance\nIn her literature, Chouldechova defines error rate balance to be the case where both groups’ false positve and false negative rates are equal. If that requirement is met, then the model satisfies error rate balance.\nFrom the previous audit of each group’s measures, we found that the FNR and FPR of the male and female groups are:\n\n# Male rates\nmfpr, mfnr\n\n(0.2270971302428256, 0.25985221674876846)\n\n\n\n# Female rates\nffpr, ffnr\n\n(0.16052631578947368, 0.2753549695740365)\n\n\nObserving that our model did not generate equal FNR and FPR for the male and female groups, our model does not satisfy error rate balance.\n\n\nStatistical Parity\nChouldechova defines stastical parity, or predictive parity, to be the case where the PPV of both groups are equal. If that requirement is met, then the model satisfies statistical parity.\n\n# Male and female PPV\nmppv, fppv\n\n(0.7729028697571744, 0.8394736842105263)\n\n\nGiven the PPV measures calculated before, it is observed that the model did not generate equal PPV for both groups as the female group had a higher PPV than the male group. Thus, our model does not satisfy statistical parity."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Here is a link to my source code: Perceptron Algorithm\n\n\nIn my implementation of the Perceptron Algorithm, I had to compute the equation:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nIn my implementation, I created a method called update that takes in three arguments, the X feature matrix, an index, and the true label y vector. Ths method computes the dot product of the X feature matrix (indexed by the ith index) and the current weight vector. Then, I take its indicator condition if the true label y multiplied by the dot product is less than 0. Lastly, to finalize the weight vector update, I multiplied the indicator condition with the product of the true label y with the indexed feature matrix X.\nThus, for each iteration of the Perceptron supposing accuracy has not reached 1, I will call the method update to update the initialized weight vector w_ by adding the result of update to the previous weight vector.\n\n\n\n\n\nHere is an experiment using a linearly separable data demonstrating the convergence towards an accuracy of 1 with the Perceptron algorithm. First, we can randomize a linearly separable data and visualize the plot.\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, we can fit the Perceptron onto its data and plot its accuracy evolution up until the algorithm has met an accuracy of 1.\n\nnp.random.seed(12345)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron has reached an accuracy of 1 just around ~650 iterations. Hence, we can plot the separating line and should discover that the line separates each point to its corresponding label.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nFor data that is non-linearly separable, we should discover that the Perceptron will not converge to perfect accuracy, but instead, the Perceptron will terminate until max iterations has reached. Below, we will produce and visualize data that is non-linearly separable.\n\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples= 100, factor=0.3, noise=0.05, random_state=0)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, similarly to the previous experiment, we can fit the Perceptron onto the data and plot the accuracy of its evolution.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Printing the last 10 values of the Perceptron history\nprint(p.history[-10:])\n\n[0.62, 0.62, 0.62, 0.51, 0.51, 0.5, 0.5, 0.54, 0.54, 0.54]\n\n\n\n# Plotting the Perceptron's score history\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron’s accuracy history shows that it never converges to 1 and oscillates up until the max iterations were reached.\n\n\n\nNow, we can experiment on the Perceptron for a set of data with at least 5 features.\n\np_features = 6\n\n# Create data that is at least 5 features\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# Fit our Perceptron onto the data \np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Visualize our Perceptron's accuracy evolution\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn the evolution of the Perceptron’s accuracy, we can see that the Perceptron never reaches an accuracy of 1, though it was pretty close. This visualization indicates that the data of n-features is not linearly separable.\n\n\n\n\nRecalling the equation for a singular perceptron update: \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nSuppose for the dot product of weight vector \\(\\tilde{w}^{(t)}\\) with n + 1 elements and vector \\(\\tilde{x_i}\\) of the same size, at most we will have a runtime complexit of \\(O(n)\\) dot product operations done. Then, for each of the dot product, we multiply that with the vector \\(\\tilde{y_i}\\) with assumptions to match the size of the two previous vectors, then that operation will also have a runtime complexity of \\(O(n)\\). The indicator function should also take \\(O(n)\\) as we are comparing a vector with length n elements with zero.\nThen, we compute the product of the vectors \\(\\tilde{y_i}\\tilde{x_i}\\) which would have n + 1 elements. Similar to the first dot product, we should expect that the runtime of the product is \\(O(n)\\). Lastly, we add what was a runtime of \\(O(n)\\) operations with the product of vectors \\(\\tilde{y_i}\\tilde{x_i}\\) — which took a runtime of \\(O(n)\\). Assuming that the addition operation is constant, we have a final runtime complexity of \\(O(n)\\) for a single iteration of a perceptron update.\nThe runtime complexity does not depend on the number of data points n, but the runtime will vary based on the number of features p. This is because as we increase the number of features, the number of multiplication operations increase exponentially as noted with the inner dot product within the indicator function."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/kernel/index.html",
    "href": "posts/kernel/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: Kernel Logistic Regression\n\n\nIn the Gradient Descent blog post, we computed the empirical risk equation: \\[L(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell({\\langle w, x_{i} \\rangle, y_{i})}\\]\nwhere the logistic loss was: \\[ \\ell({\\hat{y}, y}) = -y * log(\\sigma(\\hat{y})) - (1-y)* log(1-\\sigma(\\hat{y})) \\]\nFor the kernel logistic regression, by using the kernel trick, we are able to modify our feature matrix \\(X\\) to be of infinite-dimensional. This means that by transforming our feature matrix X to a kernel matrix, we are able to extend the binary classification of logistic regression for nonlinear features.\nNow, the empirical risk for the kernel logistic regression looks like:\n\\[L(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell({\\langle v, \\kappa({x_{i}}) \\rangle, y_{i})}\\]\nwhere \\(\\kappa({x_{i}})\\) represents the modified feature vector with row dimension \\(\\in{R^n}\\).\nTo translate the new empirical risk for the kernel logistic regression into code, I modified my empirical_risk method to take in additional parameters \\(km\\) and \\(w\\); \\(km\\) represents the kernel matrix, and \\(w\\) represents the weight vector to be optimized. Additionally, I modified my logistic loss method such that it now takes in a pre-computed y_hat as a parameter (originally, the method had taken in a y_hat using the predict method).\nWe use this loss method to compute the empirical risk by using the inner product between \\(km\\) and our weight vector \\(w\\) as our y_hat. Finally, we can take the mean of the loss of the inner product and the true label y to generate our overall empirical risk loss.\nIn my fit method, one challenge I faced was the structure of the optimized \\(w\\) weight vector. In this implementation, I had used the function scipy.optimize.minimize() to optimize my \\(w\\); the optimized \\(w\\), however, continuously generated a vector of positive values. This would cause an error in my experiments as the plot_decision_regions function will not be able to plot the trained model’s score.\nTo fix this problem, after initializing some initial vector \\(w_0\\) of dimension \\(X.shape[0]\\), I subtracted \\(w_0\\) by \\(0.5\\) to generate both negative and positive values in the optimized \\(w\\) weight vector."
  },
  {
    "objectID": "posts/kernel/index.html#basic-check",
    "href": "posts/kernel/index.html#basic-check",
    "title": "Kernel Logistic Regression",
    "section": "Basic Check",
    "text": "Basic Check\nTo test the correctness of my kernel logistic regression implementation, I can fit the model to some non-linear data to consistently get an accuracy of the training data at or above \\(90\\)%. However, there are times when the accuracy is significantly below the \\(90\\)% threshold; the function scipy.optimize.minimize() is unable to detect the gradient of the empirical risk. Thus at each iteration, the function is estimating the gradient until it reaches some “optimized” weight vector \\(w\\).\n\nfrom kernel import KernelLogisticRegression \nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nimport numpy as np \n\nnp.random.seed(12)\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(\"This is the model's accuracy: \", KLR.score(X, y))\n\nThis is the model's accuracy:  0.91"
  },
  {
    "objectID": "posts/kernel/index.html#choosing-gamma",
    "href": "posts/kernel/index.html#choosing-gamma",
    "title": "Kernel Logistic Regression",
    "section": "Choosing gamma",
    "text": "Choosing gamma\nIn the experiment above, I’ve tested my implementation of kernel logistic regression choosing a small gamma value of \\(0.1\\). However, what if I decide to use a gamma value significantly larger than \\(0.1\\)? For this experiment, let’s compare trained models with a small gamma. Let the larger gamma value be \\(10000\\), and let’s see what happens.\nShown below is some randomly generated nonlinear data of features. For this experiment, I’ve fixed the noise of the data to be \\(0.222\\).\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12)\n\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\n\nnp.random.seed(12)\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nThen, we can use this trained model on some new unseen data to test its validation accuracy.\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs shown above, the validation accuracy for the trained model with a gamma value of \\(0.1\\) shows similar performance to the testing accuracy above the \\(90\\)% accuracy threshold.\nNext, we can do the same for a model with a gamma value of \\(10000\\). In this case, we can expect that the testing accuracy to be \\(1.0\\) because the model will overfit. If the testing accuracy is \\(1.0\\), how will the validation accuracy hold on some unseen test data?\n\nnp.random.seed(15)\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nEven though our training accuracy is \\(1.0\\), our validation accuracy decreases and does not fully reach \\(1.0\\) accuracy, but the validation accuracy is still above the \\(90\\)% threshold. So, does that mean that this overfit model is still good enough? In truth, we should expect that the validation accuracy to be significantly lesser than the training accuracy. In the next experiment, we’ll vary the noise such that by observation, we can conclude that the validation accuracy will be significantly worse in generalization for an overfit model."
  },
  {
    "objectID": "posts/kernel/index.html#varying-noise",
    "href": "posts/kernel/index.html#varying-noise",
    "title": "Kernel Logistic Regression",
    "section": "Varying Noise",
    "text": "Varying Noise\nIn this experiment, we will repeat the previous experiments with varied noise. How does the choices of noise affect our initial data?\n\nnp.random.seed(15)\n\nX, y = make_moons(200, shuffle = True, noise = 0.001)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nShown above, I’ve fixed the noise to be \\(0.001\\). By observation, choosing some very small noise makes similar feature points closer to each other.\n\nnp.random.seed(16)\n\nX, y = make_moons(200, shuffle = True, noise = 0.1)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(17)\n\nX, y = make_moons(200, shuffle = True, noise = 1)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nIn the two plots above, I’ve increased the noise to be \\(0.1\\) and \\(1\\). It is observed for the crescent-like data to be increasingly nonlinear and dispersed throughout the region. Now, we can experiment how varied noise level affects the training and validation accuracy using kernel logistic regression.\nIn the previous experiment, I’ve fixed noise to be a relatively small value of \\(0.222\\). Let’s choose a lesser and greater noise value of \\(0.05\\) and \\(1\\) and observe what happens to the decision regions when plotted on the trained models. We’ll perform each model twice to record its performance for a smaller and larger noise level with a gamma level of \\(0.1\\).\n\nSmall Noise\n\nnp.random.seed(18)\n\nX, y = make_moons(200, shuffle = True, noise = 0.05)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(19)\nX, y = make_moons(200, shuffle = True, noise = 0.05)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nFor a fitted model whose gamma and noise levels are small, both training and validation accuracy are highly accurate and shows similar performance above the \\(90\\)% accuracy threshold.\n\n\nLarge Noise\n\nnp.random.seed(20)\n\nX, y = make_moons(200, shuffle = True, noise = 1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(21) \n\nX, y = make_moons(200, shuffle = True, noise = 1)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nHowever, for a model with a small gamma and a data with large noise, the model can only do so much to classify the heavily dispersed and nonlinear data. The training accuracy no longer goes at or above the \\(90\\)% threshold we’ve seen before, but considering the large noise level, a small gamma still generates moderate accuracy in classifying the features.\nFor the gamma experiment, we concluded that a smaller gamma value will produce a higher validation accuracy. However, our choice of the gamma actually depends on the choice of noise. A smaller noise will generate a higher validation accuracy than choosing a data with large noise.\n\n\nLarge Gamma and Noise\nIn the gammaexperiment, we tested an overfit model for some data of small noise and a gamma value of \\(10000\\) and observed that its validation accuracy was still above the \\(90\\)% threshold. In this experiment, I will display the results for an overfit model of data with large noise; I’ve chosen the same gamma value and set the data noise to be \\(2\\).\n\nnp.random.seed(22)\n\nX, y = make_moons(200, shuffle = True, noise = 2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(23)\n\nX, y = make_moons(200, shuffle = True, noise = 2)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nShown above, we’ve plotted the decision regions for an overfit model with a gamma value of \\(10000\\) for a data with noise \\(2\\). Though the training accuracy was \\(1.0\\), generalizing the model on unseen test data generated a validation accuracy of \\(0.5\\) which is significantly worse; the model successfully classifies and predicts for unseen test data \\(50\\)% of the time. In contrast to the previous overfit model experiment, we can conclude that choosing a large noise will significantly decrease the validation accuracy below the \\(90\\)% threshold."
  },
  {
    "objectID": "posts/kernel/index.html#other-geometries",
    "href": "posts/kernel/index.html#other-geometries",
    "title": "Kernel Logistic Regression",
    "section": "Other Geometries",
    "text": "Other Geometries\nIn the previous experiments, we’ve generated a moon shaped data. However, will our trained model using kernel logistic regression generate high validation accuracy for a circle-shaped data?\nIn this experiment, I will generate concentric circles instead of crescents using the make_circles function. Shown below are several examples of circle-shaped data with varying noise.\n\nnp.random.seed(24)\n# Small noise\nX, y = make_circles(n_samples= 200, noise=0.001, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(25)\n# Medium noise\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(26)\n# Large noise\nX, y = make_circles(n_samples= 200, noise=1, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nFor this experiment, we’ll generate a circle-shaped data with small noise and fit our model with small gamma to generate both high testing and validation accuracy. For this experiment, I’ve chosen a noise of \\(0.05\\).\n\nnp.random.seed(27)\n\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n# Test our trained model on unseen test datta\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nShown above, we observed that by choosing some small gamma and noise for a circle-shaped data, our trained model generated both high training and validation accuracy above the \\(90\\)% threshold. Thus, this is a pretty successful classifier."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This is my implementation of a ML approach for image compression with the singular value decomposition (SVD) as well as the implementation of the Laplacian spectral clustering for a spectral community detection.\n\n\n\n\n\n\nMay 3, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I talk about my learnings from Dr. Timnit Gebru’s talk on ‘Eugenics and the Promise of Utopia through Artificial General Intelligence’.\n\n\n\n\n\n\nApr 26, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my experimentation for a machine learning model onto individual characteristics given a dataset from the American Community Survey’s Public Use Microdata Sample (PUMS). Then, I’ll conduct a fairness audit to assess whether my algorithm possesses bias with respect to demographic characteristics.\n\n\n\n\n\n\nApr 3, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the least-squares linear regression with several experimentations utilizing LASSO regularization.\n\n\n\n\n\n\nMar 27, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the Kernel Logistic Regression using linear empirical risk minimization to learn nonlinear decision boundaries.\n\n\n\n\n\n\nMar 9, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation of gradient descent, a momentum method, and stochastic gradient descent for the optimization for logistic regression.\n\n\n\n\n\n\nMar 5, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the Perceptron Algorithm using numerical programming on synthetic data sets.\n\n\n\n\n\n\nFeb 23, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "Welcome to my blog!"
  }
]