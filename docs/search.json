[
  {
    "objectID": "posts/gradient/index.html",
    "href": "posts/gradient/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: https://github.com/kennyerss/kennyerss.github.io/blob/main/posts/gradient/gradient.py\n\n\nTo implement gradient descent, I created a gradient_empirical method which would calculate the gradient of the empirical risk formula. This method uses several other methods: predict, and loss_deriv. The predict method is simply a way of generating our predicted \\(y_{hat}\\) by dotting the padded feature matrix \\(X\\) with the current weight vector \\(w\\). The loss_deriv method returns the derivative of our loss function by taking our predicted \\(y_{hat}\\) and computing it on the \\(sigmoid\\) function subtracted by the true label \\(y\\) vector.\nNext, in my fit method, I used gradient_empirical on the padded feature matrix \\(X\\) with the true label vector \\(y\\) for each iteration. Using the result of my gradient empirical method, I can update the next weight vector \\(w^{k+1}\\) to be equal to \\(w^{k} - \\alpha * gradient\\). These updates continue until either the max_epochs have been reached or the loss of \\(w^{k}\\) and \\(w^{k+1}\\) no longer shows any computational difference.\n\n\n\nTo implement stochastic gradient descent with momentum, I implemented a new fit method called fit_stochastic. Inside this method, I create an optional parameter called momentum which when set to \\(True\\), initializes \\(\\beta\\) to \\(0.8\\), otherwise, sets \\(\\beta\\) to \\(0\\) which is just the regular stochastic gradient descent. Stochastic gradient is implemented by grabbing a subset of our data to compute the gradient onto with a parameter batch_size to determine the size of the subset. For every iteration of an epoch, we compute the gradient for that batch of data and update \\(w\\) by computing \\(w^{k} - \\alpha(gradient) + \\beta(w^{k} - w^{k-1})\\). I did this by storing the previous and current \\(w\\) in separate variables, initial_w and curr_w and assign initial_w = curr_w after each update of the current \\(w\\) vector. At the end of an epoch, I compute the loss and append to it to the model’s loss_history."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-1",
    "href": "posts/gradient/index.html#experiment-1",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 1",
    "text": "Experiment 1\nFor our \\(\\textbf{first experiment}\\), we will demonstrate for when the learning rate is too large, that the gradient descent method will not converge to a minimizer. Below, I’ve generated data that is at least 10 feature dimensions for this experiment.\n\nfrom gradient import LogisticRegression \nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nnp.random.seed(901)\n\n# Generate a non-linearly separable data\np_features = 11\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n# Testing the gradient descent with learning rate = 88\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\n\nLR.fit(X, y, 88, 1000)\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\n# Testing the gradient descent with learning rate = 89\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\n\nLR.fit(X, y, 89, 1000)\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nSetting the gradient descent learning rate to be 88 vs 89 to be fitted for 1000 epochs for a datatset with at least 10 features illustrated that there is a treshold for when a minimized loss is found. A possible explanation to this phenomenon could be that a large learning rate overshoots the model to miss the minimizer. Thus, the model would continue going down the direction of the gradient and may oscillate trying to find the minimizer."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-2",
    "href": "posts/gradient/index.html#experiment-2",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 2",
    "text": "Experiment 2\nFor our \\(\\textbf{ second experiment}\\), we will demonstrate the speed of convergence for different choices of batch size for the stochastic gradient algorithm.\n\n# Experimenting stochastic gradient models with different batch sizes\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 5\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 20, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 20\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 50,\n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 50\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nShown above displays the loss evolution of three stochastic gradient models of batch sized 5, 20, and 50. By observation, we can see that as the batch size increases, the model requires more epochs to converge to some minimizer solution. In this particular example, the model with batch size 5 converged the fastest, followed the model with batch size 20, and lastly, the model with batch size 50."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-3",
    "href": "posts/gradient/index.html#experiment-3",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 3",
    "text": "Experiment 3\nFor our \\(\\textbf{third experiment}\\), we will demonstrate the case in which the use of momentum significantly speeds up convergence.\n\n# Experimenting stochastic gradient models with and without momentum\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient without momentum\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = True, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with momentum\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nShown above displays the loss evolution of two stochastic models: one with momentum and one without momentum. By observation, we can conclude that the stochastic gradient model with momentum significantly speeds up convergence to some minimizer solution in comparison to the model without momentum."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Here is a link to my source code: https://github.com/kennyerss/kennyerss.github.io/blob/main/posts/perceptron/perceptron.py\n\n\nIn my implementation of the Perceptron Algorithm, I had to compute the equation:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nIn my implementation, I created a method called update that takes in three arguments, the X feature matrix, an index, and the true label y vector. Ths method computes the dot product of the X feature matrix (indexed by the ith index) and the current weight vector. Then, I take its indicator condition if the true label y multiplied by the dot product is less than 0. Lastly, to finalize the weight vector update, I multiplied the indicator condition with the product of the true label y with the indexed feature matrix X.\nThus, for each iteration of the Perceptron supposing accuracy has not reached 1, I will call the method update to update the initialized weight vector w_ by adding the result of update to the previous weight vector.\n\n\n\n\n\nHere is an experiment using a linearly separable data demonstrating the convergence towards an accuracy of 1 with the Perceptron algorithm. First, we can randomize a linearly separable data and visualize the plot.\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, we can fit the Perceptron onto its data and plot its accuracy evolution up until the algorithm has met an accuracy of 1.\n\nnp.random.seed(12345)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron has reached an accuracy of 1 just around ~650 iterations. Hence, we can plot the separating line and should discover that the line separates each point to its corresponding label.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nFor data that is non-linearly separable, we should discover that the Perceptron will not converge to perfect accuracy, but instead, the Perceptron will terminate until max iterations has reached. Below, we will produce and visualize data that is non-linearly separable.\n\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples= 100, factor=0.3, noise=0.05, random_state=0)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, similarly to the previous experiment, we can fit the Perceptron onto the data and plot the accuracy of its evolution.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Printing the last 10 values of the Perceptron history\nprint(p.history[-10:])\n\n[0.62, 0.62, 0.62, 0.51, 0.51, 0.5, 0.5, 0.54, 0.54, 0.54]\n\n\n\n# Plotting the Perceptron's score history\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron’s accuracy history shows that it never converges to 1 and oscillates up until the max iterations were reached.\n\n\n\nNow, we can experiment on the Perceptron for a set of data with at least 5 features.\n\np_features = 6\n\n# Create data that is at least 5 features\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# Fit our Perceptron onto the data \np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Visualize our Perceptron's accuracy evolution\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn the evolution of the Perceptron’s accuracy, we can see that the Perceptron never reaches an accuracy of 1, though it was pretty close. This visualization indicates that the data of n-features is not linearly separable.\n\n\n\n\nRecalling the equation for a singular perceptron update: \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nSuppose for the dot product of weight vector \\(\\tilde{w}^{(t)}\\) with n + 1 elements and vector \\(\\tilde{x_i}\\) of the same size, at most we will have a runtime complexit of \\(O(n)\\) dot product operations done. Then, for each of the dot product, we multiply that with the vector \\(\\tilde{y_i}\\) with assumptions to match the size of the two previous vectors, then that operation will also have a runtime complexity of \\(O(n)\\). The indicator function should also take \\(O(n)\\) as we are comparing a vector with length n elements with zero.\nThen, we compute the product of the vectors \\(\\tilde{y_i}\\tilde{x_i}\\) which would have n + 1 elements. Similar to the first dot product, we should expect that the runtime of the product is \\(O(n)\\). Lastly, we add what was a runtime of \\(O(n)\\) operations with the product of vectors \\(\\tilde{y_i}\\tilde{x_i}\\) — which took a runtime of \\(O(n)\\). Assuming that the addition operation is constant, we have a final runtime complexity of \\(O(n)\\) for a single iteration of a perceptron update.\nThe runtime complexity does not depend on the number of data points n, but the runtime will vary based on the number of features p. This is because as we increase the number of features, the number of multiplication operations increase exponentially as noted with the inner dot product within the indicator function."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This is my implementation of gradient descent, a momentum method, and stochastic gradient descent for the optimization for logistic regression.\n\n\n\n\n\n\nFeb 23, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the Perceptron Algorithm using numerical programming on synthetic data sets.\n\n\n\n\n\n\nFeb 23, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "Welcome to my blog!"
  }
]