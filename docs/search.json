[
  {
    "objectID": "posts/gradient/index.html",
    "href": "posts/gradient/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: Gradient Descent\n\n\nTo implement gradient descent, I created a gradient_empirical method which would calculate the gradient of the empirical risk formula. This method uses several other methods: predict, and loss_deriv. The predict method is simply a way of generating our predicted \\(y_{hat}\\) by dotting the padded feature matrix \\(X\\) with the current weight vector \\(w\\). The loss_deriv method returns the derivative of our loss function by taking our predicted \\(y_{hat}\\) and computing it on the \\(sigmoid\\) function subtracted by the true label \\(y\\) vector.\nNext, in my fit method, I used gradient_empirical on the padded feature matrix \\(X\\) with the true label vector \\(y\\) for each iteration. Using the result of my gradient empirical method, I can update the next weight vector \\(w^{k+1}\\) to be equal to \\(w^{k} - \\alpha * gradient\\). These updates continue until either the max_epochs have been reached or the loss of \\(w^{k}\\) and \\(w^{k+1}\\) no longer shows any computational difference.\n\n\n\nTo implement stochastic gradient descent with momentum, I implemented a new fit method called fit_stochastic. Inside this method, I create an optional parameter called momentum which when set to \\(True\\), initializes \\(\\beta\\) to \\(0.8\\), otherwise, sets \\(\\beta\\) to \\(0\\) which is just the regular stochastic gradient descent. Stochastic gradient is implemented by grabbing a subset of our data to compute the gradient onto with a parameter batch_size to determine the size of the subset. For every iteration of an epoch, we compute the gradient for that batch of data and update \\(w\\) by computing \\(w^{k} - \\alpha(gradient) + \\beta(w^{k} - w^{k-1})\\). I did this by storing the previous and current \\(w\\) in separate variables, initial_w and curr_w and assign initial_w = curr_w after each update of the current \\(w\\) vector. At the end of an epoch, I compute the loss and append to it to the model’s loss_history."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-1",
    "href": "posts/gradient/index.html#experiment-1",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 1",
    "text": "Experiment 1\nFor our \\(\\textbf{first experiment}\\), we will demonstrate for when the learning rate is too large, that the gradient descent method will not converge to a minimizer. Below, I’ve generated data that is at least 10 feature dimensions for this experiment.\n\nfrom gradient import LogisticRegression \nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nnp.random.seed(901)\n\n# Generate a non-linearly separable data\np_features = 11\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n# Testing the gradient descent with learning rate = 88\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\n\nLR.fit(X, y, 88, 1000)\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\n# Testing the gradient descent with learning rate = 89\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\n\nLR.fit(X, y, 89, 1000)\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nSetting the gradient descent learning rate to be 88 vs 89 to be fitted for 1000 epochs for a datatset with at least 10 features illustrated that there is a threshold for when a minimized loss is found. A possible explanation to this phenomenon could be that a large learning rate overshoots the model to miss the minimizer. Thus, the model would continue going down the direction of the gradient and may oscillate trying to find the minimizer."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-2",
    "href": "posts/gradient/index.html#experiment-2",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 2",
    "text": "Experiment 2\nFor our \\(\\textbf{ second experiment}\\), we will demonstrate the speed of convergence for different choices of batch size for the stochastic gradient algorithm.\n\n# Experimenting stochastic gradient models with different batch sizes\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 5\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 20, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 20\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 50,\n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 50\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nShown above displays the loss evolution of three stochastic gradient models of batch sized 5, 20, and 50. By observation, we can see that as the batch size increases, the model requires more epochs to converge to some minimizer solution. In this particular example, the model with batch size 5 converged the fastest, followed the model with batch size 20, and lastly, the model with batch size 50."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-3",
    "href": "posts/gradient/index.html#experiment-3",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 3",
    "text": "Experiment 3\nFor our \\(\\textbf{third experiment}\\), we will demonstrate the case in which the use of momentum significantly speeds up convergence.\n\n# Experimenting stochastic gradient models with and without momentum\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient without momentum\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = True, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with momentum\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nShown above displays the loss evolution of two stochastic models: one with momentum and one without momentum. By observation, we can conclude that the stochastic gradient model with momentum significantly speeds up convergence to some minimizer solution in comparison to the model without momentum."
  },
  {
    "objectID": "posts/linear-reg/index.html",
    "href": "posts/linear-reg/index.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Here is the link to my source code: Linear Regression\n\n\nFor this blog post, I’ve implemented least-squares linear regression in two ways: using the analytical method and an efficient gradient descent.\n\n\nThe analytical method is derived from solving the equation: \\[0 = X^{T}(X \\hat{w} - y)\\]\nwhere \\(X\\) is our paddded feature matrix. By setting the gradient equal to \\(0\\), we can solve for \\(\\hat{w}\\) to get an explicit solution for the optimized weight vector. Solving this equation requires \\(X\\) to be an invertible matrix such that it has at least many rows as columns. Thus, our final solution for \\(\\hat{w}\\) is\n\\[\\hat{w} = (X^{T}X)^{-1} X^{T}y\\]\nIn my fit_analytic method, I utilized numpy’s linalg transpose and inverse methods alongside orderly matrix multiplications to calculate the optimized weight vector \\(\\hat{w}\\).\n\n\n\nTo implement an efficient gradient descent for least-squares linear regression, instead of computing the original gradient equation at each iteration of an epoch:\n\\[\\nabla{L(w)} = X^{T}(Xw - y)\\]\nI calculated once \\(P = X^{T}X\\) and \\(q = X^{T}y\\) to reduce the time complexity of the matrix mutliplication of \\(X^{T}X\\) being \\(O(np^2)\\) and \\(X^{T}y\\) being \\(O(np)\\). Thus, reducing the gradient equation to be:\n\\[\\nabla{L(w)} = Pw - q\\]\nreduces the time complexity of calculating the gradient to be \\(O(p^2)\\) steps which is significantly faster! In my fit_gradient method, I first initialized some random weight vector of \\(p\\) shape as my padded \\(X\\) feature matrix. Then, I computed \\(P\\) and \\(q\\) which I used inside my for-loop to update my weight vector \\(self.w\\) with the gradient. At each epoch, I calculated the score of the current weight vector and appended the current score to score_history."
  },
  {
    "objectID": "posts/linear-reg/index.html#demonstration",
    "href": "posts/linear-reg/index.html#demonstration",
    "title": "Implementing Linear Regression",
    "section": "Demonstration",
    "text": "Demonstration\nShown below, I’ve generated a set of data using the LR_data method in my LinearRegression class. Then, I fit the data using both analytic and gradient descent methods and should expect a similar optimized weight vector \\(w\\).\n\nfrom linear import LinearRegression \nimport numpy as np \nfrom matplotlib import pyplot as plt\n\nLR = LinearRegression()\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nnp.random.seed(1)\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\nLR.fit_analytic(X_train, y_train) \nw = LR.w\nprint(f\"Training score = {LR.score(X_train, y_train, w).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val, w).round(4)}\")\n\nTraining score = 0.4765\nValidation score = 0.4931\n\n\n\nLR.w\n\narray([0.56552291, 0.9650224 ])\n\n\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = int(1e2))\nLR2.w\n\narray([0.56592705, 0.9648127 ])\n\n\nThen, I can plot the score_history of the gradient descent to see how the score evolved until the max iterations. By observation, the score evolved monotonically since we’re not using stochastic gradient.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(title = \"Evolution of Training Score\", xlabel = \"Iteration\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-reg/index.html#experiment-1-increasing-p_features-with-constant-n_train",
    "href": "posts/linear-reg/index.html#experiment-1-increasing-p_features-with-constant-n_train",
    "title": "Implementing Linear Regression",
    "section": "Experiment 1: Increasing p_features with constant n_train",
    "text": "Experiment 1: Increasing p_features with constant n_train\nFor this first experiment, I’ve chosen to increase the number of p_features to \\(10\\), \\(50\\), and then later choose the number of p_features to be \\(n-1\\).\n\nnp.random.seed(4)\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR1 = LinearRegression()\n\nLR1.fit_analytic(X_train, y_train) \nw = LR1.w\n\nLR1_train_score = LR1.score(X_train, y_train, w).round(4)\nLR1_validation_score = LR1.score(X_val, y_val, w).round(4)\n\n\nnp.random.seed(2)\n\np_features = 50\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR2 = LinearRegression()\n\nLR2.fit_analytic(X_train, y_train) \nw = LR2.w\n\nLR2_train_score = LR.score(X_train, y_train, w).round(4)\nLR2_validation_score = LR.score(X_val, y_val, w).round(4)\n\n\nnp.random.seed(3)\n\np_features = n_train - 1 \nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR3 = LinearRegression()\n\nLR3.fit_analytic(X_train, y_train) \nw = LR3.w\n\nLR3_train_score = LR3.score(X_train, y_train, w).round(4)\nLR3_validation_score = LR3.score(X_val, y_val, w).round(4)\n\nWe can visualize the training and validation scores to visibly observe the differences of each experiment as we increase the number of p_features.\n\n# Code from https://www.geeksforgeeks.org/bar-plot-in-matplotlib/\nbar_width = 0.2\n\ntraining_scores = [LR1_train_score, LR2_train_score, LR3_train_score]\nvalidation_scores = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(training_scores))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br1, training_scores, width = bar_width,\n        edgecolor ='grey', label ='training score')\nplt.bar(br2, validation_scores, width = bar_width,\n        edgecolor ='grey', label ='validation score')\n\nplt.title('Number of p_features vs score with constant n_train')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(training_scores))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\nAs shown on the graph above, we can see that as the number of p_features increases up to n_train - 1, the fitted model’s training_score also increases. However, the model’s validation score decreases. This conclusion is related to the model being overfit due to the significant difference of the training and validation score between the model with n_train - 1 p_features. This means that we’ve trained the model exactly to some random training data given, however, when validated on the true labels, the calculated optimized weight vector \\(w\\) will be highly inaccurate in comparison to the labels."
  },
  {
    "objectID": "posts/linear-reg/index.html#experiment-2-lasso-regularization",
    "href": "posts/linear-reg/index.html#experiment-2-lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "Experiment 2: LASSO Regularization",
    "text": "Experiment 2: LASSO Regularization\nUsing LASSO regularization, we modify the original loss function to add a regularization term:\n\\[L(w) = ||Xw - y||^{2}_{2} + \\alpha||w'||_{1}\\]\nThis extension of the regularization term minimizes the weight vector \\(w\\) as small as it could be and forces the weight vector’s entries to be exactly zero.\n\nVarying Degrees of Alpha\nFor this experiment, we can choose varying degrees of alpha while increasing the number of p_features of our data.\n\nfrom sklearn.linear_model import Lasso\n\n\n# Alpha of 0.001\nL1 = Lasso(alpha = 0.001)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_1 = L1.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_2 = L1.score(X_val, y_val)\n\n\np_features = n_train - 1 \n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_3 = L1.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.001\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L1_validation_1, L1_validation_2, L1_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='linear validation score')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso validation score')\n\nplt.title('Number of p_features vs Validation Scores for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\n\n# Alpha of 0.01\nL2 = Lasso(alpha = 0.01)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_1 = L2.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_2 = L2.score(X_val, y_val)\n\n\np_features = n_train - 1\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_3 = L2.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.01\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L2_validation_1, L2_validation_2, L2_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='linear validation score')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso validation score')\n\nplt.title('Number of p_features vs Validation Scores for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\n\n# Alpha of 0.1\nL3 = Lasso(alpha = 0.1)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_1 = L3.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_2 = L3.score(X_val, y_val)\n\n\np_features = n_train - 1\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_3 = L3.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.1\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L3_validation_1, L3_validation_2, L3_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='linear validation score')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso validation score')\n\nplt.title('Number of p_features vs Validation Scores for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\n\n\n\n\n\n# Plotting all three alpha levels\n\nbar_width = 0.2\n\nlasso_validation_1 = [L1_validation_1, L1_validation_2, L1_validation_3]\nlasso_validation_2 = [L2_validation_1, L2_validation_2, L2_validation_3]\nlasso_validation_3 = [L3_validation_1, L3_validation_2, L3_validation_3]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation_1))\nbr2 = [x + bar_width for x in br1]\nbr3 = [x + bar_width for x in br2]\n\n# Make the plot\nplt.bar(br1, lasso_validation_1, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.001')\nplt.bar(br2, lasso_validation_2, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.01')\nplt.bar(br3, lasso_validation_3, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.1')\n\nplt.title('Number of p_features vs Validation Scores for Lasso Regularization with Varying Alpha')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\n\n\n\n\nAfter plotting all three degrees of alpha with increasing number of p_features up to n_train - 1, I found that smaller values of alpha (alpha = \\(0.001\\)) will still retain a moderately high validation score despite reaching up to n_train - 1 number of p_features. However, as I increase the degree of alpha to \\(0.01\\) and \\(0.1\\), the difference in validation score between LASSO regularization and standard linear regression becomes significantly different. As I increase the strength of the regularizer, the validation score for LASSO regularization approaches zero, and is no longer accuracte in predicting the true labels.\nIn conclusion, LASSO regularization can improve a model’s validation score with smaller alpha levels in contrast to utilizing standard linear regression even with up to n_train - 1 number of p_features. However, as you increase the strength of the regularization, the validation score decreases significantly, and the model is no longer proficient in its predictions."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Here is a link to my source code: Perceptron Algorithm\n\n\nIn my implementation of the Perceptron Algorithm, I had to compute the equation:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nIn my implementation, I created a method called update that takes in three arguments, the X feature matrix, an index, and the true label y vector. Ths method computes the dot product of the X feature matrix (indexed by the ith index) and the current weight vector. Then, I take its indicator condition if the true label y multiplied by the dot product is less than 0. Lastly, to finalize the weight vector update, I multiplied the indicator condition with the product of the true label y with the indexed feature matrix X.\nThus, for each iteration of the Perceptron supposing accuracy has not reached 1, I will call the method update to update the initialized weight vector w_ by adding the result of update to the previous weight vector.\n\n\n\n\n\nHere is an experiment using a linearly separable data demonstrating the convergence towards an accuracy of 1 with the Perceptron algorithm. First, we can randomize a linearly separable data and visualize the plot.\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, we can fit the Perceptron onto its data and plot its accuracy evolution up until the algorithm has met an accuracy of 1.\n\nnp.random.seed(12345)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron has reached an accuracy of 1 just around ~650 iterations. Hence, we can plot the separating line and should discover that the line separates each point to its corresponding label.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nFor data that is non-linearly separable, we should discover that the Perceptron will not converge to perfect accuracy, but instead, the Perceptron will terminate until max iterations has reached. Below, we will produce and visualize data that is non-linearly separable.\n\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples= 100, factor=0.3, noise=0.05, random_state=0)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, similarly to the previous experiment, we can fit the Perceptron onto the data and plot the accuracy of its evolution.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Printing the last 10 values of the Perceptron history\nprint(p.history[-10:])\n\n[0.62, 0.62, 0.62, 0.51, 0.51, 0.5, 0.5, 0.54, 0.54, 0.54]\n\n\n\n# Plotting the Perceptron's score history\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron’s accuracy history shows that it never converges to 1 and oscillates up until the max iterations were reached.\n\n\n\nNow, we can experiment on the Perceptron for a set of data with at least 5 features.\n\np_features = 6\n\n# Create data that is at least 5 features\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# Fit our Perceptron onto the data \np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Visualize our Perceptron's accuracy evolution\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn the evolution of the Perceptron’s accuracy, we can see that the Perceptron never reaches an accuracy of 1, though it was pretty close. This visualization indicates that the data of n-features is not linearly separable.\n\n\n\n\nRecalling the equation for a singular perceptron update: \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nSuppose for the dot product of weight vector \\(\\tilde{w}^{(t)}\\) with n + 1 elements and vector \\(\\tilde{x_i}\\) of the same size, at most we will have a runtime complexit of \\(O(n)\\) dot product operations done. Then, for each of the dot product, we multiply that with the vector \\(\\tilde{y_i}\\) with assumptions to match the size of the two previous vectors, then that operation will also have a runtime complexity of \\(O(n)\\). The indicator function should also take \\(O(n)\\) as we are comparing a vector with length n elements with zero.\nThen, we compute the product of the vectors \\(\\tilde{y_i}\\tilde{x_i}\\) which would have n + 1 elements. Similar to the first dot product, we should expect that the runtime of the product is \\(O(n)\\). Lastly, we add what was a runtime of \\(O(n)\\) operations with the product of vectors \\(\\tilde{y_i}\\tilde{x_i}\\) — which took a runtime of \\(O(n)\\). Assuming that the addition operation is constant, we have a final runtime complexity of \\(O(n)\\) for a single iteration of a perceptron update.\nThe runtime complexity does not depend on the number of data points n, but the runtime will vary based on the number of features p. This is because as we increase the number of features, the number of multiplication operations increase exponentially as noted with the inner dot product within the indicator function."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/kernel/index.html",
    "href": "posts/kernel/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: Kernel Logistic Regression\n\n\nIn the Gradient Descent blog post, we computed the empirical risk equation: \\[L(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell({\\langle w, x_{i} \\rangle, y_{i})}\\]\nwhere the logistic loss was: \\[ \\ell({\\hat{y}, y}) = -y * log(\\sigma(\\hat{y})) - (1-y)* log(1-\\sigma(\\hat{y})) \\]\nFor the kernel logistic regression, by using the kernel trick, we are able to modify our feature matrix \\(X\\) to be of infinite-dimensional. This means that by transforming our feature matrix X to a kernel matrix, we are able to extend the binary classification of logistic regression for nonlinear features.\nNow, the empirical risk for the kernel logistic regression looks like:\n\\[L(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell({\\langle v, \\kappa({x_{i}}) \\rangle, y_{i})}\\]\nwhere \\(\\kappa({x_{i}})\\) represents the modified feature vector with row dimension \\(\\in{R^n}\\).\nTo translate the new empirical risk for the kernel logistic regression into code, I modified my empirical_risk method to take in additional parameters \\(km\\) and \\(w\\); \\(km\\) represents the kernel matrix, and \\(w\\) represents the weight vector to be optimized. Additionally, I modified my logistic loss method such that it now takes in a pre-computed y_hat as a parameter (originally, the method had taken in a y_hat using the predict method).\nWe use this loss method to compute the empirical risk by using the inner product between \\(km\\) and our weight vector \\(w\\) as our y_hat. Finally, we can take the mean of the loss of the inner product and the true label y to generate our overall empirical risk loss.\nIn my fit method, one challenge I faced was the structure of the optimized \\(w\\) weight vector. In this implementation, I had used the function scipy.optimize.minimize() to optimize my \\(w\\); the optimized \\(w\\), however, continuously generated a vector of positive values. This would cause an error in my experiments as the plot_decision_regions function will not be able to plot the trained model’s score.\nTo fix this problem, after initializing some initial vector \\(w_0\\) of dimension \\(X.shape[0]\\), I subtracted \\(w_0\\) by \\(0.5\\) to generate both negative and positive values in the optimized \\(w\\) weight vector."
  },
  {
    "objectID": "posts/kernel/index.html#basic-check",
    "href": "posts/kernel/index.html#basic-check",
    "title": "Kernel Logistic Regression",
    "section": "Basic Check",
    "text": "Basic Check\nTo test the correctness of my kernel logistic regression implementation, I can fit the model to some non-linear data to consistently get an accuracy of the training data at or above \\(90\\)%. However, there are times when the accuracy is significantly below the \\(90\\)% threshold; the function scipy.optimize.minimize() is unable to detect the gradient of the empirical risk. Thus at each iteration, the function is estimating the gradient until it reaches some “optimized” weight vector \\(w\\).\n\nfrom kernel import KernelLogisticRegression \nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nimport numpy as np \n\nnp.random.seed(12)\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(\"This is the model's accuracy: \", KLR.score(X, y))\n\nThis is the model's accuracy:  0.91"
  },
  {
    "objectID": "posts/kernel/index.html#choosing-gamma",
    "href": "posts/kernel/index.html#choosing-gamma",
    "title": "Kernel Logistic Regression",
    "section": "Choosing gamma",
    "text": "Choosing gamma\nIn the experiment above, I’ve tested my implementation of kernel logistic regression choosing a small gamma value of \\(0.1\\). However, what if I decide to use a gamma value significantly larger than \\(0.1\\)? For this experiment, let’s compare trained models with a small gamma. Let the larger gamma value be \\(10000\\), and let’s see what happens.\nShown below is some randomly generated nonlinear data of features. For this experiment, I’ve fixed the noise of the data to be \\(0.222\\).\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12)\n\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\n\nnp.random.seed(12)\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nThen, we can use this trained model on some new unseen data to test its validation accuracy.\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs shown above, the validation accuracy for the trained model with a gamma value of \\(0.1\\) shows similar performance to the testing accuracy above the \\(90\\)% accuracy threshold.\nNext, we can do the same for a model with a gamma value of \\(10000\\). In this case, we can expect that the testing accuracy to be \\(1.0\\) because the model will overfit. If the testing accuracy is \\(1.0\\), how will the validation accuracy hold on some unseen test data?\n\nnp.random.seed(15)\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nEven though our training accuracy is \\(1.0\\), our validation accuracy decreases and does not fully reach \\(1.0\\) accuracy, but the validation accuracy is still above the \\(90\\)% threshold. So, does that mean that this overfit model is still good enough? In truth, we should expect that the validation accuracy to be significantly lesser than the training accuracy. In the next experiment, we’ll vary the noise such that by observation, we can conclude that the validation accuracy will be significantly worse in generalization for an overfit model."
  },
  {
    "objectID": "posts/kernel/index.html#varying-noise",
    "href": "posts/kernel/index.html#varying-noise",
    "title": "Kernel Logistic Regression",
    "section": "Varying Noise",
    "text": "Varying Noise\nIn this experiment, we will repeat the previous experiments with varied noise. How does the choices of noise affect our initial data?\n\nnp.random.seed(15)\n\nX, y = make_moons(200, shuffle = True, noise = 0.001)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nShown above, I’ve fixed the noise to be \\(0.001\\). By observation, choosing some very small noise makes similar feature points closer to each other.\n\nnp.random.seed(16)\n\nX, y = make_moons(200, shuffle = True, noise = 0.1)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(17)\n\nX, y = make_moons(200, shuffle = True, noise = 1)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nIn the two plots above, I’ve increased the noise to be \\(0.1\\) and \\(1\\). It is observed for the crescent-like data to be increasingly nonlinear and dispersed throughout the region. Now, we can experiment how varied noise level affects the training and validation accuracy using kernel logistic regression.\nIn the previous experiment, I’ve fixed noise to be a relatively small value of \\(0.222\\). Let’s choose a lesser and greater noise value of \\(0.05\\) and \\(1\\) and observe what happens to the decision regions when plotted on the trained models. We’ll perform each model twice to record its performance for a smaller and larger noise level with a gamma level of \\(0.1\\).\n\nSmall Noise\n\nnp.random.seed(18)\n\nX, y = make_moons(200, shuffle = True, noise = 0.05)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(19)\nX, y = make_moons(200, shuffle = True, noise = 0.05)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nFor a fitted model whose gamma and noise levels are small, both training and validation accuracy are highly accurate and shows similar performance above the \\(90\\)% accuracy threshold.\n\n\nLarge Noise\n\nnp.random.seed(20)\n\nX, y = make_moons(200, shuffle = True, noise = 1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(21) \n\nX, y = make_moons(200, shuffle = True, noise = 1)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nHowever, for a model with a small gamma and a data with large noise, the model can only do so much to classify the heavily dispersed and nonlinear data. The training accuracy no longer goes at or above the \\(90\\)% threshold we’ve seen before, but considering the large noise level, a small gamma still generates moderate accuracy in classifying the features.\nFor the gamma experiment, we concluded that a smaller gamma value will produce a higher validation accuracy. However, our choice of the gamma actually depends on the choice of noise. A smaller noise will generate a higher validation accuracy than choosing a data with large noise.\n\n\nLarge Gamma and Noise\nIn the gammaexperiment, we tested an overfit model for some data of small noise and a gamma value of \\(10000\\) and observed that its validation accuracy was still above the \\(90\\)% threshold. In this experiment, I will display the results for an overfit model of data with large noise; I’ve chosen the same gamma value and set the data noise to be \\(2\\).\n\nnp.random.seed(22)\n\nX, y = make_moons(200, shuffle = True, noise = 2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(23)\n\nX, y = make_moons(200, shuffle = True, noise = 2)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nShown above, we’ve plotted the decision regions for an overfit model with a gamma value of \\(10000\\) for a data with noise \\(2\\). Though the training accuracy was \\(1.0\\), generalizing the model on unseen test data generated a validation accuracy of \\(0.5\\) which is significantly worse; the model successfully classifies and predicts for unseen test data \\(50\\)% of the time. In contrast to the previous overfit model experiment, we can conclude that choosing a large noise will significantly decrease the validation accuracy below the \\(90\\)% threshold."
  },
  {
    "objectID": "posts/kernel/index.html#other-geometries",
    "href": "posts/kernel/index.html#other-geometries",
    "title": "Kernel Logistic Regression",
    "section": "Other Geometries",
    "text": "Other Geometries\nIn the previous experiments, we’ve generated a moon shaped data. However, will our trained model using kernel logistic regression generate high validation accuracy for a circle-shaped data?\nIn this experiment, I will generate concentric circles instead of crescents using the make_circles function. Shown below are several examples of circle-shaped data with varying noise.\n\nnp.random.seed(24)\n# Small noise\nX, y = make_circles(n_samples= 200, noise=0.001, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(25)\n# Medium noise\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(26)\n# Large noise\nX, y = make_circles(n_samples= 200, noise=1, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nFor this experiment, we’ll generate a circle-shaped data with small noise and fit our model with small gamma to generate both high testing and validation accuracy. For this experiment, I’ve chosen a noise of \\(0.05\\).\n\nnp.random.seed(27)\n\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n# Test our trained model on unseen test datta\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nShown above, we observed that by choosing some small gamma and noise for a circle-shaped data, our trained model generated both high training and validation accuracy above the \\(90\\)% threshold. Thus, this is a pretty successful classifier."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This is my implementation for the least-squares linear regression with several experimentations utilizing LASSO regularization.\n\n\n\n\n\n\nMar 27, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the Kernel Logistic Regression using linear empirical risk minimization to learn nonlinear decision boundaries.\n\n\n\n\n\n\nMar 9, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation of gradient descent, a momentum method, and stochastic gradient descent for the optimization for logistic regression.\n\n\n\n\n\n\nMar 5, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the Perceptron Algorithm using numerical programming on synthetic data sets.\n\n\n\n\n\n\nFeb 23, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "Welcome to my blog!"
  }
]