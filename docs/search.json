[
  {
    "objectID": "posts/gradient/index.html",
    "href": "posts/gradient/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "# fit the model\n\n# np.random.seed(123)\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.001, max_epochs = 10000)\n\n# inspect the fitted value of w\nLR.w \n\nprint(LR.loss_history[-10:]) #just the last few values\n\nMax epochs reached\n[0.23992083185841037, 0.2399177901182996, 0.23991474898155554, 0.23991170844801563, 0.2399086685175172, 0.23990562918989775, 0.23990259046499482, 0.23989955234264607, 0.23989651482268903, 0.23989347790496146]\n\n\n\n# add a constant feature to the feature matrix\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Here is a link to my source code: https://github.com/kennyerss/kennyerss.github.io/blob/main/posts/perceptron/perceptron.py\n\n\nIn my implementation of the Perceptron Algorithm, I had to compute the equation:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nIn my implementation, I created a method called update that takes in three arguments, the X feature matrix, an index, and the true label y vector. Ths method computes the dot product of the X feature matrix (indexed by the ith index) and the current weight vector. Then, I take its indicator condition if the true label y multiplied by the dot product is less than 0. Lastly, to finalize the weight vector update, I multiplied the indicator condition with the product of the true label y with the indexed feature matrix X.\nThus, for each iteration of the Perceptron supposing accuracy has not reached 1, I will call the method update to update the initialized weight vector w_ by adding the result of update to the previous weight vector.\n\n\n\n\n\nHere is an experiment using a linearly separable data demonstrating the convergence towards an accuracy of 1 with the Perceptron algorithm. First, we can randomize a linearly separable data and visualize the plot.\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nThen, we can fit the Perceptron onto its data and plot its accuracy evolution up until the algorithm has met an accuracy of 1.\n\nnp.random.seed(12345)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron has reached an accuracy of 1 just around ~650 iterations. Hence, we can plot the separating line and should discover that the line separates each point to its corresponding label.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nFor data that is non-linearly separable, we should discover that the Perceptron will not converge to perfect accuracy, but instead, the Perceptron will terminate until max iterations has reached. Below, we will produce and visualize data that is non-linearly separable.\n\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples= 100, factor=0.3, noise=0.05, random_state=0)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, similarly to the previous experiment, we can fit the Perceptron onto the data and plot the accuracy of its evolution.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Printing the last 10 values of the Perceptron history\nprint(p.history[-10:])\n\n[0.42, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.32, 0.5, 0.35]\n\n\n\n# Plotting the Perceptron's score history\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron’s accuracy history shows that it never converges to 1 and oscillates up until the max iterations were reached.\n\n\n\nNow, we can experiment on the Perceptron for a set of data with at least 5 features.\n\np_features = 6\n\n# Create data that is at least 5 features\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# Fit our Perceptron onto the data \np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Visualize our Perceptron's accuracy evolution\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn the evolution of the Perceptron’s accuracy, we can see that the Perceptron never reaches an accuracy of 1, though it was pretty close. This visualization indicates that the data of n-features is not linearly separable.\n\n\n\n\nRecalling the equation for a singular perceptron update: \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nSuppose for the dot product of weight vector \\(\\tilde{w}^{(t)}\\) with n + 1 elements and vector \\(\\tilde{x_i}\\) of the same size, at most we will have a runtime complexit of \\(O(n)\\) dot product operations done. Then, for each of the dot product, we multiply that with the vector \\(\\tilde{y_i}\\) with assumptions to match the size of the two previous vectors, then that operation will also have a runtime complexity of \\(O(n)\\). The indicator function should also take \\(O(n)\\) as we are comparing a vector with length n elements with zero.\nThen, we compute the product of the vectors \\(\\tilde{y_i}\\tilde{x_i}\\) which would have n + 1 elements. Similar to the first dot product, we should expect that the runtime of the product is \\(O(n)\\). Lastly, we add what was a runtime of \\(O(n)\\) operations with the product of vectors \\(\\tilde{y_i}\\tilde{x_i}\\) — which took a runtime of \\(O(n)\\). Assuming that the addition operation is constant, we have a final runtime complexity of \\(O(n)\\) for a single iteration of a perceptron update.\nThe runtime complexity does not depend on the number of data points n, but the runtime will vary based on the number of features p. This is because as we increase the number of features, the number of multiplication operations increase exponentially as noted with the inner dot product within the indicator function."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This is my implementation for the Perceptron Algorithm using numerical programming on synthetic data sets.\n\n\n\n\n\n\nFeb 23, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "Welcome to my blog!"
  },
  {
    "objectID": "warmups/convex-linear-models-demo.html",
    "href": "warmups/convex-linear-models-demo.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "z = np.linspace(0, 5, 101)\nplt.plot(z, -np.log(1/(1 + np.exp(-z)))) \nlabs = plt.gca().set(xlabel = r\"$\\hat{y}$\", ylabel = r\"$-\\log \\sigma(\\hat{y})$\")\n\n\n\n\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nNote that this data is not linearly separable. The perceptron algorithm wouldn’t even have converged for this data set, but logistic regression will do great.\n\n\n\n\n\n# add a constant feature to the feature matrix\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nNow we’ll define some functions to compute the empirical risk:\n\n# implement: (WE ARE CODING TOGETHER HERE!!)\n# - predict\n# - sigmoid\n# - logistic_loss\n# - empirical_risk\n\ndef predict(X, w):\n    return X@w\n\ndef logistic_loss(y_hat, y):\n    return -y*np.log(sigmoid(y_hat)) - (1-y)*np.log(1 - sigmoid(y_hat))\n\ndef sigmoid(y_hat):\n    return 1 / (1 + np.exp(-y_hat))\n\ndef empirical_risk(X, y, w, loss):\n    # Pass the choice of loss function as an argument\n    y_hat = predict(X, w)\n    return loss(y_hat, y).mean() # Compare true prediction with true label and take the average of losses\n\n\nFinally, we can write the function that will solve the empirical risk minimization problem for us. We’re going to use the scipy.optimize.minimize function, which is a built-in function for solving minimization problems. Soon, we’ll study how to solve minimization problems from scratch.\nThe scipy.optimize.minimize function requires us to pass it a single function that accepts a vector of parameters, plus an initial guess for the parameters.\n\ndef find_pars(X, y):\n    \n    p = X.shape[1]\n    w0 = np.random.rand(p) # random initial guess\n    \n    # perform the minimization\n    result = minimize(lambda w: empirical_risk(X, y, w, logistic_loss), \n                      x0 = w0) \n    \n    # return the parameters\n    return result.x\n\nOk, let’s try it and take a look at the parameters we obtained. Because the final column of X_ is the constant column of 1s, the final entry of w is interpretable as the intercept term b.\n\nw = find_pars(X_, y)\nw\n\narray([ 2.66396796,  3.35446292, -0.04226593])\n\n\nAnd, finally, we can plot the linear classifier that we learned.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\nplt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\n\n\n\n\nSince the logistic loss is convex, we are guaranteed that this solution is the unique best solution (as measured by the logistic loss). There is no other possible set of parameters that would lead to a better result (again, as measured by the logistic loss)."
  },
  {
    "objectID": "warmups/convexity.html",
    "href": "warmups/convexity.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "# Part 3 \n\n%matplotlib inline\nimport math\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\n# Define domain of our graph\nx_ = np.linspace(-10, 10)\nfig, ax = plt.subplots(1, 1) \nfig, bx = plt.subplots(1,1)\n\n# Make functions to graph \ndef f_(z): return -1*np.log(z)\ndef g_(z): return -1*np.log(1-z)\n\n# Set axes labels\nax.set(xlabel = \"z\", \n       ylabel = \"f(z)\")\nbx.set(xlabel = \"z\",\n       ylabel = \"g(z)\")\n\n# Plot functions\nax.plot(x_, f_(x_))\nbx.plot(x_, g_(x_))\n\n/var/folders/b3/2h6lz8x90ys7r0tpcdh03l400000gn/T/ipykernel_37473/3133112784.py:14: RuntimeWarning: invalid value encountered in log\n  def f_(z): return -1*np.log(z)\n/var/folders/b3/2h6lz8x90ys7r0tpcdh03l400000gn/T/ipykernel_37473/3133112784.py:15: RuntimeWarning: invalid value encountered in log\n  def g_(z): return -1*np.log(1-z)"
  },
  {
    "objectID": "warmups/gradient-warmup.html",
    "href": "warmups/gradient-warmup.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np \nnp.random.seed(123)\n\ndef f(w):\n    return np.sin(w[0]*w[1])\n\n\ndef gradient(w_0):\n    # Split guess vector into two coords\n    x1 = w_0[0]\n    x2 = w_0[1]\n    \n    # Returns the gradient of function\n    first_gradient = x2 * np.cos(x1 * x2)\n    second_gradient = x1 * np.cos(x1 * x2)\n    return np.array(first_gradient, second_gradient)\n\nalpha = 0.001\nmax_steps = 100000\n\n# Initialize random guess\nw_0 = np.random.rand(2)\nw_1 = np.random.rand(2)\n\nprint(\"Random guess 1: \" + str(w_0))\nprint(\"Random guess 2: \" + str(w_1))\n\n\nfor _ in range(max_steps):\n\n    # Compute gradient at current location\n    grdnt = gradient(w_0)\n    grdnt_2 = gradient(w_1)\n    \n    if np.isclose(grdnt, 0) and np.isclose(grdnt_2, 0):\n        print(\"Minimizer found\")\n        break\n        \n    # Take a step down the gradient\n    w_0 -= alpha * grdnt\n    w_1 -= alpha * grdnt_2\n    \nprint(\"Found minimizer 1: \" + str(w_0))\nprint(\"Found minimizer 2: \" + str(w_1))\n\nRandom guess 1: [0.69646919 0.28613933]\nRandom guess 2: [0.22685145 0.55131477]\nMinimizer found\nFound minimizer 1: [4.10329856e-01 5.22073246e-09]\nFound minimizer 2: [-3.24463306e-01  9.99748784e-09]"
  },
  {
    "objectID": "warmups/perceptron.html",
    "href": "warmups/perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#### Perceptron\n\n# Part 2\n\nimport numpy as np\nnp.random.seed(1234)\n\ndef perceptron_classify(w, b, x):\n    '''\n    Given 1d numpy arrays w and x and a scalar b\n    \n    Returns 0 if the dot product of w and x is less than b\n    Returns 1 if the dot product of w and x is greater than or equal to b \n    \n    ex: w = [1,2,3], b = 6, x = [3,4,5] => returns 0 for values of b less than sum of w@x (26) else 1 \n    '''\n    dot_product = w@x # dot product notation\n    #return 1 if np.sum(dot_product) >= b else 0 \n    # Optimized version\n    return (x@w - b > 0) * 1 \n\nn = 5\nA = np.random.randint(1, 10, size = (n, n))\nB = np.random.randint(1, 5, size = n)\nb = 24\n\nC = np.array([1,2,3])\nD = np.array([3,4,5])\nprint(np.sum(C@D))\nperceptron_classify(C, b, D)\n\n26\n\n\n1"
  }
]