[
  {
    "objectID": "posts/gradient/index.html",
    "href": "posts/gradient/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: Gradient Descent\n\n\nTo implement gradient descent, I created a gradient_empirical method which would calculate the gradient of the empirical risk formula. This method uses several other methods: predict, and loss_deriv. The predict method is simply a way of generating our predicted \\(y_{hat}\\) by dotting the padded feature matrix \\(X\\) with the current weight vector \\(w\\). The loss_deriv method returns the derivative of our loss function by taking our predicted \\(y_{hat}\\) and computing it on the \\(sigmoid\\) function subtracted by the true label \\(y\\) vector.\nNext, in my fit method, I used gradient_empirical on the padded feature matrix \\(X\\) with the true label vector \\(y\\) for each iteration. Using the result of my gradient empirical method, I can update the next weight vector \\(w^{k+1}\\) to be equal to \\(w^{k} - \\alpha * gradient\\). These updates continue until either the max_epochs have been reached or the loss of \\(w^{k}\\) and \\(w^{k+1}\\) no longer shows any computational difference.\n\n\n\nTo implement stochastic gradient descent with momentum, I implemented a new fit method called fit_stochastic. Inside this method, I create an optional parameter called momentum which when set to \\(True\\), initializes \\(\\beta\\) to \\(0.8\\), otherwise, sets \\(\\beta\\) to \\(0\\) which is just the regular stochastic gradient descent. Stochastic gradient is implemented by grabbing a subset of our data to compute the gradient onto with a parameter batch_size to determine the size of the subset. For every iteration of an epoch, we compute the gradient for that batch of data and update \\(w\\) by computing \\(w^{k} - \\alpha(gradient) + \\beta(w^{k} - w^{k-1})\\). I did this by storing the previous and current \\(w\\) in separate variables, initial_w and curr_w and assign initial_w = curr_w after each update of the current \\(w\\) vector. At the end of an epoch, I compute the loss and append to it to the model’s loss_history."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-1",
    "href": "posts/gradient/index.html#experiment-1",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 1",
    "text": "Experiment 1\nFor our \\(\\textbf{first experiment}\\), we will demonstrate for when the learning rate is too large, that the gradient descent method will not converge to a minimizer. Below, I’ve generated data that is at least 10 feature dimensions for this experiment.\n\nfrom gradient import LogisticRegression \nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nnp.random.seed(901)\n\n# Generate a non-linearly separable data\np_features = 11\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n# Testing the gradient descent with learning rate = 88\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\n\nLR.fit(X, y, 88, 1000)\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\n# Testing the gradient descent with learning rate = 89\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\n\nLR.fit(X, y, 89, 1000)\nloss = LR.empirical_risk(X_, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nSetting the gradient descent learning rate to be 88 vs 89 to be fitted for 1000 epochs for a datatset with at least 10 features illustrated that there is a threshold for when a minimized loss is found. A possible explanation to this phenomenon could be that a large learning rate overshoots the model to miss the minimizer. Thus, the model would continue going down the direction of the gradient and may oscillate trying to find the minimizer."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-2",
    "href": "posts/gradient/index.html#experiment-2",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 2",
    "text": "Experiment 2\nFor our \\(\\textbf{ second experiment}\\), we will demonstrate the speed of convergence for different choices of batch size for the stochastic gradient algorithm.\n\n# Experimenting stochastic gradient models with different batch sizes\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 5\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 20, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 20\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 50,\n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size of 50\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nShown above displays the loss evolution of three stochastic gradient models of batch sized 5, 20, and 50. By observation, we can see that as the batch size increases, the model requires more epochs to converge to some minimizer solution. In this particular example, the model with batch size 5 converged the fastest, followed the model with batch size 20, and lastly, the model with batch size 50."
  },
  {
    "objectID": "posts/gradient/index.html#experiment-3",
    "href": "posts/gradient/index.html#experiment-3",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 3",
    "text": "Experiment 3\nFor our \\(\\textbf{third experiment}\\), we will demonstrate the case in which the use of momentum significantly speeds up convergence.\n\n# Experimenting stochastic gradient models with and without momentum\n\nnp.random.seed(901)\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient without momentum\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = True, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with momentum\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nShown above displays the loss evolution of two stochastic models: one with momentum and one without momentum. By observation, we can conclude that the stochastic gradient model with momentum significantly speeds up convergence to some minimizer solution in comparison to the model without momentum."
  },
  {
    "objectID": "posts/linear-reg/index.html",
    "href": "posts/linear-reg/index.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Here is the link to my source code: Linear Regression\n\n\nFor this blog post, I’ve implemented least-squares linear regression in two ways: using the analytical method and an efficient gradient descent.\n\n\nThe analytical method is derived from solving the equation: \\[0 = X^{T}(X \\hat{w} - y)\\]\nwhere \\(X\\) is our paddded feature matrix. By setting the gradient equal to \\(0\\), we can solve for \\(\\hat{w}\\) to get an explicit solution for the optimized weight vector. Solving this equation requires \\(X\\) to be an invertible matrix such that it has at least many rows as columns. Thus, our final solution for \\(\\hat{w}\\) is\n\\[\\hat{w} = (X^{T}X)^{-1} X^{T}y\\]\nIn my fit_analytic method, I utilized numpy’s linalg transpose and inverse methods alongside orderly matrix multiplications to calculate the optimized weight vector \\(\\hat{w}\\).\n\n\n\nTo implement an efficient gradient descent for least-squares linear regression, instead of computing the original gradient equation at each iteration of an epoch:\n\\[\\nabla{L(w)} = X^{T}(Xw - y)\\]\nI calculated once \\(P = X^{T}X\\) and \\(q = X^{T}y\\) to reduce the time complexity of the matrix mutliplication of \\(X^{T}X\\) being \\(O(np^2)\\) and \\(X^{T}y\\) being \\(O(np)\\). Thus, reducing the gradient equation to be:\n\\[\\nabla{L(w)} = Pw - q\\]\nreduces the time complexity of calculating the gradient to be \\(O(p^2)\\) steps which is significantly faster! In my fit_gradient method, I first initialized some random weight vector of \\(p\\) shape as my padded \\(X\\) feature matrix. Then, I computed \\(P\\) and \\(q\\) which I used inside my for-loop to update my weight vector \\(self.w\\) with the gradient. At each epoch, I calculated the score of the current weight vector and appended the current score to score_history."
  },
  {
    "objectID": "posts/linear-reg/index.html#demonstration",
    "href": "posts/linear-reg/index.html#demonstration",
    "title": "Implementing Linear Regression",
    "section": "Demonstration",
    "text": "Demonstration\nShown below, I’ve generated a set of data using the LR_data method in my LinearRegression class. Then, I fit the data using both analytic and gradient descent methods and should expect a similar optimized weight vector \\(w\\).\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom linear import LinearRegression \nimport numpy as np \nfrom matplotlib import pyplot as plt\n\nLR = LinearRegression()\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nnp.random.seed(1)\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\nLR.fit_analytic(X_train, y_train) \nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.4765\nValidation score = 0.4931\n\n\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = int(1e2))\n\nThen, I can plot the score_history of the gradient descent to see how the score evolved until the max iterations. By observation, the score evolved monotonically since we’re not using stochastic gradient.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(title = \"Evolution of Training Score\", xlabel = \"Iteration\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-reg/index.html#experiment-1-increasing-p_features-with-constant-n_train",
    "href": "posts/linear-reg/index.html#experiment-1-increasing-p_features-with-constant-n_train",
    "title": "Implementing Linear Regression",
    "section": "Experiment 1: Increasing p_features with constant n_train",
    "text": "Experiment 1: Increasing p_features with constant n_train\nFor this first experiment, I’ve chosen to increase the number of p_features to \\(10\\), \\(50\\), and then later choose the number of p_features to be \\(n-1\\).\n\nnp.random.seed(4)\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR1 = LinearRegression()\n\nLR1.fit_analytic(X_train, y_train) \n\nLR1_train_score = LR1.score(X_train, y_train).round(4)\nLR1_validation_score = LR1.score(X_val, y_val).round(4)\n\n\nnp.random.seed(2)\n\np_features = 50\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR2 = LinearRegression()\n\nLR2.fit_analytic(X_train, y_train) \n\nLR2.fit_analytic(X_train, y_train) \n\nLR2_train_score = LR2.score(X_train, y_train).round(4)\nLR2_validation_score = LR2.score(X_val, y_val).round(4)\n\n\nnp.random.seed(3)\n\np_features = n_train - 1 \nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n# Use our fit_analytic method to get training and validation score\nLR3 = LinearRegression()\n\nLR3.fit_analytic(X_train, y_train) \n\nLR3_train_score = LR3.score(X_train, y_train).round(4)\nLR3_validation_score = LR3.score(X_val, y_val).round(4)\n\nWe can visualize the training and validation scores to visibly observe the differences of each experiment as we increase the number of p_features.\n\n# Code from https://www.geeksforgeeks.org/bar-plot-in-matplotlib/\nbar_width = 0.2\n\ntraining_scores = [LR1_train_score, LR2_train_score, LR3_train_score]\nvalidation_scores = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(training_scores))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br1, training_scores, width = bar_width,\n        edgecolor ='grey', label ='training score')\nplt.bar(br2, validation_scores, width = bar_width,\n        edgecolor ='grey', label ='validation score')\n\nplt.title('Number of p_features vs score with constant n_train')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(training_scores))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\nAs shown on the graph above, we can see that as the number of p_features increases up to n_train - 1, the fitted model’s training_score also increases. However, the model’s validation score decreases. This conclusion is related to the model being overfit due to the significant difference of the training and validation score between the model with n_train - 1 p_features. This means that we’ve trained the model exactly to some random training data given, however, when validated on the true labels, the calculated optimized weight vector \\(w\\) will be highly inaccurate in comparison to the labels."
  },
  {
    "objectID": "posts/linear-reg/index.html#experiment-2-lasso-regularization",
    "href": "posts/linear-reg/index.html#experiment-2-lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "Experiment 2: LASSO Regularization",
    "text": "Experiment 2: LASSO Regularization\nUsing LASSO regularization, we modify the original loss function to add a regularization term:\n\\[L(w) = ||Xw - y||^{2}_{2} + \\alpha||w'||_{1}\\]\nThis extension of the regularization term minimizes the weight vector \\(w\\) as small as it could be and forces the weight vector’s entries to be exactly zero.\n\nVarying Degrees of Alpha\nFor this experiment, we can choose varying degrees of alpha while increasing the number of p_features of our data.\n\nfrom sklearn.linear_model import Lasso\n\n\n# Alpha of 0.001\nL1 = Lasso(alpha = 0.001)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_1 = L1.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_2 = L1.score(X_val, y_val)\n\n\np_features = n_train - 1 \n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL1.fit(X_train, y_train)\nL1_validation_3 = L1.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.001\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L1_validation_1, L1_validation_2, L1_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='standard linear regression')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso regularization')\n\nplt.title('Number of p_features vs Validation Score for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Validation Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\n\n# Alpha of 0.01\nL2 = Lasso(alpha = 0.01)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_1 = L2.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_2 = L2.score(X_val, y_val)\n\n\np_features = n_train - 1\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2_validation_3 = L2.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.01\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L2_validation_1, L2_validation_2, L2_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='standard linear regression')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso regularization score')\n\nplt.title('Number of p_features vs Validation Score for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Validation Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\nplt.show()\n\n\n\n\n\n# Alpha of 0.1\nL3 = Lasso(alpha = 0.1)\n\n\np_features = 10\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_1 = L3.score(X_val, y_val)\n\n\np_features = 50\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_2 = L3.score(X_val, y_val)\n\n\np_features = n_train - 1\n\n# Fit our model\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL3.fit(X_train, y_train)\nL3_validation_3 = L3.score(X_val, y_val)\n\nAfter fitting the Lasso model with an alpha of \\(0.1\\), we can plot its validation scores in contrast to the standard linear regression.\n\nbar_width = 0.2\n\nlasso_validation = [L3_validation_1, L3_validation_2, L3_validation_3]\nlinear_validation = [LR1_validation_score, LR2_validation_score, LR3_validation_score]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation))\nbr2 = [x + bar_width for x in br1]\n\n# Make the plot\nplt.bar(br2, linear_validation, width = bar_width,\n        edgecolor ='grey', label ='standard linear regression')\nplt.bar(br1, lasso_validation, width = bar_width,\n        edgecolor ='grey', label ='lasso regularization')\n\nplt.title('Number of p_features vs Validation Scores for Lasso Regularization and Standard Linear Regression')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Validation Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\n\n\n\n\n\n# Plotting all three alpha levels\n\nbar_width = 0.2\n\nlasso_validation_1 = [L1_validation_1, L1_validation_2, L1_validation_3]\nlasso_validation_2 = [L2_validation_1, L2_validation_2, L2_validation_3]\nlasso_validation_3 = [L3_validation_1, L3_validation_2, L3_validation_3]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(lasso_validation_1))\nbr2 = [x + bar_width for x in br1]\nbr3 = [x + bar_width for x in br2]\n\n# Make the plot\nplt.bar(br1, lasso_validation_1, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.001')\nplt.bar(br2, lasso_validation_2, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.01')\nplt.bar(br3, lasso_validation_3, width = bar_width,\n        edgecolor ='grey', label ='alpha = 0.1')\n\nplt.title('Number of p_features vs Validation Score for Lasso Regularization with Varying Alpha Degrees')\n\n# Adding Xticks\nplt.xlabel('Number of p_features', fontsize = 12)\nplt.ylabel('Validation Score', fontsize = 12)\nplt.xticks([r + 0.1 for r in range(len(lasso_validation))],\n        ['10', '50', 'n_train - 1'])\n\nlegend = plt.legend()\n\n\n\n\nAfter plotting all three degrees of alpha with increasing number of p_features up to n_train - 1, I found that smaller values of alpha (alpha = \\(0.001\\)) will still retain a moderately high validation score despite reaching up to n_train - 1 number of p_features. However, as I increase the degree of alpha to \\(0.01\\) and \\(0.1\\), the difference in validation score between LASSO regularization and standard linear regression becomes significantly different. As I increase the strength of the regularizer, the validation score for LASSO regularization approaches zero, and is no longer accuracte in predicting the true labels.\nIn conclusion, LASSO regularization can improve a model’s validation score with smaller alpha levels in contrast to utilizing standard linear regression even with up to n_train - 1 number of p_features. However, as you increase the strength of the regularization, the validation score decreases significantly, and the model is no longer proficient in its predictions."
  },
  {
    "objectID": "posts/audit/index.html",
    "href": "posts/audit/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "For this analysis, I’ve chosen to pull the 2018 PUMS data from the state of Illinois.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"IL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\n\n# Reducing number of features\npossible_features=['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'RAC1P', 'SEX', 'PINCP']\nacs_data[possible_features]\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      POBP\n      RELP\n      WKHP\n      RAC1P\n      SEX\n      PINCP\n    \n  \n  \n    \n      0\n      86\n      NaN\n      16.0\n      2\n      NaN\n      17\n      16\n      NaN\n      1\n      2\n      22200.0\n    \n    \n      1\n      30\n      1.0\n      18.0\n      5\n      4220.0\n      22\n      17\n      99.0\n      1\n      2\n      15000.0\n    \n    \n      2\n      57\n      NaN\n      19.0\n      5\n      NaN\n      17\n      17\n      NaN\n      2\n      1\n      0.0\n    \n    \n      3\n      69\n      1.0\n      19.0\n      3\n      1551.0\n      20\n      16\n      NaN\n      1\n      1\n      21600.0\n    \n    \n      4\n      18\n      3.0\n      18.0\n      5\n      2350.0\n      6\n      17\n      NaN\n      2\n      1\n      350.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      126451\n      72\n      NaN\n      16.0\n      1\n      NaN\n      17\n      1\n      NaN\n      1\n      1\n      68800.0\n    \n    \n      126452\n      43\n      1.0\n      21.0\n      1\n      710.0\n      210\n      0\n      40.0\n      6\n      1\n      89000.0\n    \n    \n      126453\n      39\n      NaN\n      21.0\n      1\n      NaN\n      210\n      1\n      NaN\n      6\n      2\n      0.0\n    \n    \n      126454\n      7\n      NaN\n      4.0\n      5\n      NaN\n      210\n      2\n      NaN\n      6\n      2\n      NaN\n    \n    \n      126455\n      5\n      NaN\n      1.0\n      5\n      NaN\n      210\n      2\n      NaN\n      6\n      1\n      NaN\n    \n  \n\n126456 rows × 11 columns\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"PINCP\", \"SEX\"]]\n\nBelow is a preprocessing step to filter our data in corresponding to the literature’s filters over the variables AGEP (age), PINCP (total income), WKHP (usual hours worked in the last 12 months), and PWGTP (individual’s weight).\n\n# source code taken from folktables PUMS problem constructions\ndef adult_filter(data):\n    \"\"\"Mimic the filters in place for Adult data.\n    Adult documentation notes: Extraction was done by Barry Becker from\n    the 1994 Census database. A set of reasonably clean records was extracted\n    using the following conditions:\n    ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n    \"\"\"\n    df = data\n    df = df[df['AGEP'] > 16]\n    df = df[df['PINCP'] > 100]\n    df = df[df['WKHP'] > 0]\n    df = df[df['PWGTP'] >= 1]\n    return df\n\n\n# Transform our IncomeProblem to a BasicProblem to minimize features we would like to use\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: 1*(x > 50000),\n    group='SEX',\n    preprocess= adult_filter,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n\n# Split our data into test and train sets \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)"
  },
  {
    "objectID": "posts/audit/index.html#basic-descriptives",
    "href": "posts/audit/index.html#basic-descriptives",
    "title": "Auditing Allocative Bias",
    "section": "Basic Descriptives",
    "text": "Basic Descriptives\n\n# Convert our data into a dataframe for visualization\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\ndf\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      POBP\n      RELP\n      WKHP\n      RAC1P\n      group\n      label\n    \n  \n  \n    \n      0\n      48.0\n      1.0\n      20.0\n      1.0\n      3515.0\n      17.0\n      1.0\n      40.0\n      1.0\n      2\n      0\n    \n    \n      1\n      49.0\n      1.0\n      21.0\n      1.0\n      850.0\n      17.0\n      1.0\n      40.0\n      1.0\n      1\n      1\n    \n    \n      2\n      44.0\n      1.0\n      1.0\n      1.0\n      4251.0\n      303.0\n      12.0\n      40.0\n      1.0\n      1\n      0\n    \n    \n      3\n      46.0\n      1.0\n      23.0\n      1.0\n      440.0\n      17.0\n      0.0\n      40.0\n      1.0\n      2\n      1\n    \n    \n      4\n      36.0\n      1.0\n      16.0\n      1.0\n      5610.0\n      17.0\n      1.0\n      60.0\n      1.0\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53607\n      32.0\n      1.0\n      20.0\n      1.0\n      8740.0\n      17.0\n      0.0\n      48.0\n      1.0\n      1\n      0\n    \n    \n      53608\n      75.0\n      3.0\n      22.0\n      1.0\n      2320.0\n      17.0\n      0.0\n      7.0\n      1.0\n      1\n      0\n    \n    \n      53609\n      53.0\n      1.0\n      16.0\n      3.0\n      4220.0\n      17.0\n      10.0\n      6.0\n      1.0\n      2\n      0\n    \n    \n      53610\n      57.0\n      3.0\n      16.0\n      1.0\n      7000.0\n      17.0\n      1.0\n      50.0\n      1.0\n      1\n      0\n    \n    \n      53611\n      37.0\n      3.0\n      17.0\n      1.0\n      3945.0\n      17.0\n      0.0\n      40.0\n      1.0\n      1\n      1\n    \n  \n\n53612 rows × 11 columns\n\n\n\nIn this dataset, there are 53612 total individuals present.\n\n# Calculate the proportion of individuals who have an income of over $50k\ndf[\"label\"].mean()\n\n0.39455718868909945\n\n\nAdditionally, of the 53612 individuals in the data, only about \\(39.5\\)% of the individuas have an income of over $50k.\n\n# Calculate the total individuals per group\ngroup_prop = df.groupby(\"group\", as_index = False).size()\ngroup_prop.rename(columns = {\"size\" : \"total\"})\n\n\n\n\n\n  \n    \n      \n      group\n      total\n    \n  \n  \n    \n      0\n      1\n      27724\n    \n    \n      1\n      2\n      25888\n    \n  \n\n\n\n\nIn this data, there are a total of 27724 males and 25888 females identifying folks.\n\ngroup_label = df.groupby([\"group\"], as_index = False)[\"label\"].mean()\ngroup_label\n\n\n\n\n\n  \n    \n      \n      group\n      label\n    \n  \n  \n    \n      0\n      1\n      0.478611\n    \n    \n      1\n      2\n      0.304543\n    \n  \n\n\n\n\nIn the male group, about \\(47.9\\)% of male individuals have an income of over 50k. Whereas in the female group, only about \\(30.5\\)% of female individuals have an income of over 50k.\nHere, we can visualize the intersectional relationship between the proportion of individuals in the SEX and RAC1P groups who have an income of over $50k.\n\n# Compute proportion of positive labels by race and sex\nrace_prop = df.groupby([\"RAC1P\", \"group\"], as_index = False)[\"label\"].mean()\n\n\n# Visualize positive labels for both and race\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport seaborn.objects as so\n\nsns.catplot(data=race_prop, x=\"RAC1P\", y=\"label\", hue=\"group\", kind=\"bar\")\n\n# Add labels to title, x and y axis\nplt.title(\"Distribution of Race and Sex with Income Over $50K\")\nplt.xlabel(\"Race Group\")\nplt.ylabel(\"Proportion of Individuals with Income Over $50K\")\n\nplt.show()\n\n\n\n\nShown above is a visualization of the different racial groups studied and their proportion of individuals who have an income of over $50k grouped by sex in Illinois.\nOne major observation from the plot is that the male sex has a larger proportion of individuals who have an income of over 50k than the female sex for all race groups with the exceptions to race groups 4 and 7 — Alaska Natives alone and Native Hawaiian and Other Pacific Islanders, respectively. Alaska Natives alone are also outliers in this plot as both sex groups show zero proportion of all individuals who have an income of over 50k in Illinois; similarly, females in the race group 5 (American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races) show zero proportion of all individuals who have an income of over 50k.\nAll RAC1P groups and other variables used in this analysis are described in the Appendix here."
  },
  {
    "objectID": "posts/audit/index.html#tuning-complexity-of-decisiontree",
    "href": "posts/audit/index.html#tuning-complexity-of-decisiontree",
    "title": "Auditing Allocative Bias",
    "section": "Tuning Complexity of DecisionTree",
    "text": "Tuning Complexity of DecisionTree\nNow that we’ve explored our data, we can train a machine learning model to predict the individuals living in Illinos who have an income of over $50k. Using the DecisionTree model, I can tune the depth using a cross-validation of 10-fold to choose a tree that maximizes the accuracy.\nFor each level complexity of our tree, we can assess its performance and verify which depth we should use for our model.\n\n# From Classifier Lecture\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1)\n\n# Plotting each of the complexity and its performance \nfor d in range(2, 15):\n    T = DecisionTreeClassifier(max_depth = d)\n    m = cross_val_score(T, X_train, y_train, cv = 10).mean()\n    ax.scatter(d, m, color = \"black\")\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", \n              ylabel = \"Performance (score)\", \n              title = \"Performance (score) vs Complexity (depth) of DecisionTreeClassifier\")\n\n\n\n\nFrom the plot, it is observed that a complexity of roughly 11 is the best for the data set which maximizes the performance of the DecisionTreeClassifier.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nWe can now train our model and fit it over the testing set:\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = 11))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=11))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=11))])StandardScalerStandardScaler()DecisionTreeClassifierDecisionTreeClassifier(max_depth=11)"
  },
  {
    "objectID": "posts/audit/index.html#overall-measures",
    "href": "posts/audit/index.html#overall-measures",
    "title": "Auditing Allocative Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\n\nOverall Accuracy\n\n# Generate predictions based off model\ny_hat = model.predict(X_test)\n\nThe overall accuracy in predicting whether an individual has an income of over $50k is:\n\ntotal_acc = (y_hat == y_test).mean()\ntotal_acc\n\n0.7805132796180245\n\n\nThus, our model achieved a \\(78\\)% accuracy in predicting whether a individual has an income of over $50k in Illinois.\n\n\nPositive Predictive Value (PPV)\n\nfrom sklearn.metrics import confusion_matrix\n# Compute the confusion matrix for the model\ncm = confusion_matrix(y_test, y_hat, normalize = \"true\")\n\n# Extract PPV, FPR, and FNR\nppv = cm[0][0]\nfpr = cm[0][1]\nfnr = cm[1][0]\n\ncm\n\narray([[0.80999511, 0.19000489],\n       [0.26570881, 0.73429119]])\n\n\nThe positive predictive value (PPV) can be calculated given the equation:\n\\[PPV = \\frac{TP}{TP + FP}\\]\nFortunately, our confusion matrix does this calculation for us! As shown in the top left corner of the confusion matrix of the overall PPV of our model is about \\(81\\)%.\nHence, of the total population of the dataset, about \\(81\\)% of individuals who reside in Illinois who does have an income of over $50k is true.\n\n\nFalse Negative Rate (FNR) and False Positive Rate (FPR)\nSimilarly, we can extract the overall FNR and FPR of our model without having to compute its respective formulas using the confusion matrix. The FNR (shown in the bottom left corner) is about \\(27\\)%, and the FPR (shown in the top righ corner) is about \\(19\\)%.\nOur model has a FNR of about \\(27\\)% which suggests that the model is classifying individuals with income of over $50k as having an income below the threshold nearly a third of the time.\nFurthermore, our model has a FPR of about \\(19\\)% which suggests that the model is classifying individuals with income below $50k as having an income above the threshold about 19% of the time.\nNow that we’ve audited our overall model, we can check if similar measures are observed for both male and female groups."
  },
  {
    "objectID": "posts/audit/index.html#by-group-measures",
    "href": "posts/audit/index.html#by-group-measures",
    "title": "Auditing Allocative Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\nWe can further our audit analysis by checking the previous measures for each group male and female who makes an income of over $50k.\n\nMale Individuals\nHere is the confusion matrix for male individuals in Illinois who make an income of over $50k:\n\nmcm = confusion_matrix(y_test[group_test == 1], y_hat[group_test == 1], normalize = \"true\")\n\n# Extract PPV, FPR, and FNR\nmppv = mcm[0][0]\nmfpr = mcm[0][1]\nmfnr = mcm[1][0]\n\nmcm\n\narray([[0.77290287, 0.22709713],\n       [0.25985222, 0.74014778]])\n\n\n\nMale Accuracy\n\nmale_acc = (y_hat == y_test)[group_test == 1].mean()\nmale_acc\n\n0.7574214202561118\n\n\nOur model correctly predicts whether a male individual in Illinois has an income of over $50k about 76% of the time.\n\n\nPositive Predictive Value (PPV)\nThe PPV of my model for male individuals is about 77% which suggests that for this population, our model correctly classifies male individuals in Illinois who does have an income of over $50k (true positives) 77% of the time.\n\n\nFalse Negative Rate (FNR) and False Positive Rate (FPR)\nThe FNR and FPR of my model for male individuals is about \\(26\\)% and \\(23\\)%, respectively.\nHence, our model does not classify the true positives (male individuals who have an income of over $50k) 26% of the time; in other words, 26% of male individuals who have an income above the 50k threshold are classified as having an income below the threshold.\nLastly, given the FPR of nearly 23%, our model incorrectly classifies male individuals who have an income below $50k as true positives nearly 23% of the time.\n\n\n\nFemale Individuals\nNow, let’s check the probability measures for female individuals. Shown below is the confusion matrix for female individuals who make an income of over $50k:\n\nfcm = confusion_matrix(y_test[group_test == 2], y_hat[group_test == 2], normalize = \"true\")\n\n# Extract PPV, FPR, and FNR\nfppv = fcm[0][0]\nffpr = fcm[0][1]\nffnr = fcm[1][0]\n\nfcm\n\narray([[0.83947368, 0.16052632],\n       [0.27535497, 0.72464503]])\n\n\n\nFemale Accuracy\nThe accuracy for predicting whether a female individual in Illinois has an income of over $50k is:\n\nfemale_acc = (y_hat == y_test)[group_test == 2].mean()\nfemale_acc\n\n0.8048071034905082\n\n\nOur model correctly predicts whether a female individual has an income of over $50k about 80% of the time.\n\n\nPositive Predictive Value (PPV)\nObserving the confusion matrix for the female group, we can see that the PPV of our model is about \\(84\\)%.\nHence, our model classifies the true positives, female individuals who does have an income of over $50k, as true nearly 84% of the time.\n\n\nFalse Negative Rate (FNR) and False Positive Rate (FPR)\nOur model displays the FNR and FPR for female individuals to be about 27% and 16%, respectively.\nHence, our model does not classify the true positives (female individuals who have an income of over $50k) 27% of the time; in other words, 27% of female individuals who have an income above the 50k threshold are classified as having an income below the threshold.\nLastly, given the FPR of about 16%, our model incorrectly classifies female individuals who have an income below $50k as true positives about 16% of the time.\n\n\nComparison\nAfter auditing for both groups, we observe that both male and female groups have a similar FNR which suggests that the model is incorrectly missing a similar proportion of individuals in Illinois who have an income above $50k (but slightly higher for female individuals).\nHowever, the FPR for the female group is lower than the male group. Thus, our model is generating fewer false positives for female individuals than male individuals; in other words, for female individuals who have an income below $50k, our model will classify them to belong in the negative class (individuals who have an income below 50k) about 84% of the time — while male individuals are only classified to belong in the negative class about 77% of the time.\nShown below is a visualization of the model’s overall measures and of each subgroup’s.\n\nbar_width = 0.2\n\ntotal = [total_acc, ppv, fpr, fnr]\nmale = [male_acc, mppv, mfpr, mfnr]\nfemale = [female_acc, fppv, ffpr, ffnr]\n\n\n# Set position of bar on X axis\nbr1 = np.arange(len(total))\nbr2 = [x + bar_width for x in br1]\nbr3 = [x + bar_width for x in br2]\n\n# Make the plot\nplt.bar(br1, total, width = bar_width,\n        edgecolor ='grey', label ='overall')\nplt.bar(br2, male, width = bar_width,\n        edgecolor ='grey', label ='male')\nplt.bar(br3, female, width = bar_width,\n        edgecolor ='grey', label ='female')\n\nplt.title(\"Distribution of Model's Overall and By-Group Measures\")\n\n# Adding Xticks\nplt.xlabel('Measures', fontsize = 12)\nplt.ylabel('Probability', fontsize = 12)\nplt.xticks([r + 0.2 for r in range(len(total))],\n           ['Accuracy', 'PPV', 'FPR', 'FNR'])\n\nlegend = plt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/audit/index.html#bias-measures",
    "href": "posts/audit/index.html#bias-measures",
    "title": "Auditing Allocative Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\n\nCalibration\nNow, we can assess whether our model is reasonably calibrated by grouping by the average of both group and the true_label over the predicted_label. For this analysis, a calibrated model means that the fraction of individuals who have an income of over $50k is the same across both the male and female groups.\nTo do this, I first created a new data frame containing my group (SEX), true_label (y_test), and my predicted labels (y_hat). Then, I mutated the “group” column to Male (1) and Female (2) for visual readability.\n\n# Create a dataframe containing the true and predicted labels and groups\ncalib_features = [\"group\", \"true_label\", \"predicted_label\"]\ncalib_df = pd.DataFrame({\"group\": group_test, \"true_label\": y_test, \"predicted_label\": y_hat}, columns = calib_features)\n\n# Mutate df group values for readability using .replace()\ngroup_test = calib_df[\"group\"].replace([1, 2], [\"Male\", \"Female\"])\ncalib_df = pd.DataFrame({\"group\": group_test, \"true_label\": y_test, \"predicted_label\": y_hat}, columns = calib_features)\ncalib_df.head()\n\n\n\n\n\n  \n    \n      \n      group\n      true_label\n      predicted_label\n    \n  \n  \n    \n      0\n      Female\n      0\n      0\n    \n    \n      1\n      Male\n      0\n      1\n    \n    \n      2\n      Male\n      0\n      0\n    \n    \n      3\n      Male\n      0\n      1\n    \n    \n      4\n      Female\n      1\n      1\n    \n  \n\n\n\n\nAfter creating the new dataframe, I generated two visualizations: a line plot and bar plot of the mean across the groups’ predicted labels.\n\n# Perform calibration test using .groupby() method\nmeans = calib_df.groupby([\"group\", \"true_label\"])[\"predicted_label\"].mean().reset_index(name = \"mean\")\n\n# Visualizations of \ng = sns.lineplot(data = means, x = \"true_label\", y = \"mean\", hue = \"group\")\n\nplt.title(\"Line Plot of Mean Across Sub-groups Labels\")\nplt.xlabel(\"True Label\")\nplt.ylabel(\"Mean\")\nplt.show()\n\n\n\n\n\np = sns.barplot(data = means, x = \"true_label\", y = \"mean\", hue = \"group\")\n\nplt.title(\"Distribution of Mean Across Group Labels\")\nplt.xlabel(\"True Label\")\nplt.ylabel(\"Mean\")\nplt.show()\n\n\n\n\nFrom the perspective of calibration, the model might suggest that it is biased in the direction of the Male group. Out of those who were predicted to have an income of over $50k, slightly more male individuals were predicted to have an income over the income threshold in comparison to Female individuals.\nAdditionally, the line plot shows that across the ranges of true labels, male individuals maintain a higher statistical mean and proportion who are predicted to make an income of over $50k than female individuals. Thus, both plots can argue that the model is biased in the direction of the male group.\nHowever, since the rates for both groups are sufficiently close, it may also be correct to suggest that the model appears well-calibrated.\n\n\nError Rate Balance\nIn her literature, Chouldechova defines error rate balance to be the case where both groups’ false positve and false negative rates are equal. If that requirement is met, then the model satisfies error rate balance.\nFrom the previous audit of each group’s measures, we found that the FNR and FPR of the male and female groups are:\n\n# Male rates\nmfpr, mfnr\n\n(0.2270971302428256, 0.25985221674876846)\n\n\n\n# Female rates\nffpr, ffnr\n\n(0.16052631578947368, 0.2753549695740365)\n\n\nObserving that our model did not generate equal FNR and FPR for the male and female groups, our model does not satisfy error rate balance.\n\n\nStatistical Parity\nChouldechova defines stastical parity, or predictive parity, to be the case where the PPV of both groups are equal. If that requirement is met, then the model satisfies statistical parity.\n\n# Male and female PPV\nmppv, fppv\n\n(0.7729028697571744, 0.8394736842105263)\n\n\nGiven the PPV measures calculated before, it is observed that the model did not generate equal PPV for both groups as the female group had a higher PPV than the male group. Thus, our model does not satisfy statistical parity."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Here is a link to my source code: Perceptron Algorithm\n\n\nIn my implementation of the Perceptron Algorithm, I had to compute the equation:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nIn my implementation, I created a method called update that takes in three arguments, the X feature matrix, an index, and the true label y vector. Ths method computes the dot product of the X feature matrix (indexed by the ith index) and the current weight vector. Then, I take its indicator condition if the true label y multiplied by the dot product is less than 0. Lastly, to finalize the weight vector update, I multiplied the indicator condition with the product of the true label y with the indexed feature matrix X.\nThus, for each iteration of the Perceptron supposing accuracy has not reached 1, I will call the method update to update the initialized weight vector w_ by adding the result of update to the previous weight vector.\n\n\n\n\n\nHere is an experiment using a linearly separable data demonstrating the convergence towards an accuracy of 1 with the Perceptron algorithm. First, we can randomize a linearly separable data and visualize the plot.\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, we can fit the Perceptron onto its data and plot its accuracy evolution up until the algorithm has met an accuracy of 1.\n\nnp.random.seed(12345)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron has reached an accuracy of 1 just around ~650 iterations. Hence, we can plot the separating line and should discover that the line separates each point to its corresponding label.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nFor data that is non-linearly separable, we should discover that the Perceptron will not converge to perfect accuracy, but instead, the Perceptron will terminate until max iterations has reached. Below, we will produce and visualize data that is non-linearly separable.\n\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples= 100, factor=0.3, noise=0.05, random_state=0)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThen, similarly to the previous experiment, we can fit the Perceptron onto the data and plot the accuracy of its evolution.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Printing the last 10 values of the Perceptron history\nprint(p.history[-10:])\n\n[0.62, 0.62, 0.62, 0.51, 0.51, 0.5, 0.5, 0.54, 0.54, 0.54]\n\n\n\n# Plotting the Perceptron's score history\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nAs shown above, the Perceptron’s accuracy history shows that it never converges to 1 and oscillates up until the max iterations were reached.\n\n\n\nNow, we can experiment on the Perceptron for a set of data with at least 5 features.\n\np_features = 6\n\n# Create data that is at least 5 features\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# Fit our Perceptron onto the data \np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n# Visualize our Perceptron's accuracy evolution\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn the evolution of the Perceptron’s accuracy, we can see that the Perceptron never reaches an accuracy of 1, though it was pretty close. This visualization indicates that the data of n-features is not linearly separable.\n\n\n\n\nRecalling the equation for a singular perceptron update: \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y_i}\\langle{\\tilde{w}^{(t)},\\tilde{x_i}} \\rangle < 0)\\tilde{y_i}\\tilde{x_i}\\]\nSuppose for the dot product of weight vector \\(\\tilde{w}^{(t)}\\) with n + 1 elements and vector \\(\\tilde{x_i}\\) of the same size, at most we will have a runtime complexit of \\(O(n)\\) dot product operations done. Then, for each of the dot product, we multiply that with the vector \\(\\tilde{y_i}\\) with assumptions to match the size of the two previous vectors, then that operation will also have a runtime complexity of \\(O(n)\\). The indicator function should also take \\(O(n)\\) as we are comparing a vector with length n elements with zero.\nThen, we compute the product of the vectors \\(\\tilde{y_i}\\tilde{x_i}\\) which would have n + 1 elements. Similar to the first dot product, we should expect that the runtime of the product is \\(O(n)\\). Lastly, we add what was a runtime of \\(O(n)\\) operations with the product of vectors \\(\\tilde{y_i}\\tilde{x_i}\\) — which took a runtime of \\(O(n)\\). Assuming that the addition operation is constant, we have a final runtime complexity of \\(O(n)\\) for a single iteration of a perceptron update.\nThe runtime complexity does not depend on the number of data points n, but the runtime will vary based on the number of features p. This is because as we increase the number of features, the number of multiplication operations increase exponentially as noted with the inner dot product within the indicator function."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/kernel/index.html",
    "href": "posts/kernel/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: Kernel Logistic Regression\n\n\nIn the Gradient Descent blog post, we computed the empirical risk equation: \\[L(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell({\\langle w, x_{i} \\rangle, y_{i})}\\]\nwhere the logistic loss was: \\[ \\ell({\\hat{y}, y}) = -y * log(\\sigma(\\hat{y})) - (1-y)* log(1-\\sigma(\\hat{y})) \\]\nFor the kernel logistic regression, by using the kernel trick, we are able to modify our feature matrix \\(X\\) to be of infinite-dimensional. This means that by transforming our feature matrix X to a kernel matrix, we are able to extend the binary classification of logistic regression for nonlinear features.\nNow, the empirical risk for the kernel logistic regression looks like:\n\\[L(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell({\\langle v, \\kappa({x_{i}}) \\rangle, y_{i})}\\]\nwhere \\(\\kappa({x_{i}})\\) represents the modified feature vector with row dimension \\(\\in{R^n}\\).\nTo translate the new empirical risk for the kernel logistic regression into code, I modified my empirical_risk method to take in additional parameters \\(km\\) and \\(w\\); \\(km\\) represents the kernel matrix, and \\(w\\) represents the weight vector to be optimized. Additionally, I modified my logistic loss method such that it now takes in a pre-computed y_hat as a parameter (originally, the method had taken in a y_hat using the predict method).\nWe use this loss method to compute the empirical risk by using the inner product between \\(km\\) and our weight vector \\(w\\) as our y_hat. Finally, we can take the mean of the loss of the inner product and the true label y to generate our overall empirical risk loss.\nIn my fit method, one challenge I faced was the structure of the optimized \\(w\\) weight vector. In this implementation, I had used the function scipy.optimize.minimize() to optimize my \\(w\\); the optimized \\(w\\), however, continuously generated a vector of positive values. This would cause an error in my experiments as the plot_decision_regions function will not be able to plot the trained model’s score.\nTo fix this problem, after initializing some initial vector \\(w_0\\) of dimension \\(X.shape[0]\\), I subtracted \\(w_0\\) by \\(0.5\\) to generate both negative and positive values in the optimized \\(w\\) weight vector."
  },
  {
    "objectID": "posts/kernel/index.html#basic-check",
    "href": "posts/kernel/index.html#basic-check",
    "title": "Kernel Logistic Regression",
    "section": "Basic Check",
    "text": "Basic Check\nTo test the correctness of my kernel logistic regression implementation, I can fit the model to some non-linear data to consistently get an accuracy of the training data at or above \\(90\\)%. However, there are times when the accuracy is significantly below the \\(90\\)% threshold; the function scipy.optimize.minimize() is unable to detect the gradient of the empirical risk. Thus at each iteration, the function is estimating the gradient until it reaches some “optimized” weight vector \\(w\\).\n\nfrom kernel import KernelLogisticRegression \nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nimport numpy as np \n\nnp.random.seed(12)\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(\"This is the model's accuracy: \", KLR.score(X, y))\n\nThis is the model's accuracy:  0.91"
  },
  {
    "objectID": "posts/kernel/index.html#choosing-gamma",
    "href": "posts/kernel/index.html#choosing-gamma",
    "title": "Kernel Logistic Regression",
    "section": "Choosing gamma",
    "text": "Choosing gamma\nIn the experiment above, I’ve tested my implementation of kernel logistic regression choosing a small gamma value of \\(0.1\\). However, what if I decide to use a gamma value significantly larger than \\(0.1\\)? For this experiment, let’s compare trained models with a small gamma. Let the larger gamma value be \\(10000\\), and let’s see what happens.\nShown below is some randomly generated nonlinear data of features. For this experiment, I’ve fixed the noise of the data to be \\(0.222\\).\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12)\n\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\n\nnp.random.seed(12)\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nThen, we can use this trained model on some new unseen data to test its validation accuracy.\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs shown above, the validation accuracy for the trained model with a gamma value of \\(0.1\\) shows similar performance to the testing accuracy above the \\(90\\)% accuracy threshold.\nNext, we can do the same for a model with a gamma value of \\(10000\\). In this case, we can expect that the testing accuracy to be \\(1.0\\) because the model will overfit. If the testing accuracy is \\(1.0\\), how will the validation accuracy hold on some unseen test data?\n\nnp.random.seed(15)\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.222)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nEven though our training accuracy is \\(1.0\\), our validation accuracy decreases and does not fully reach \\(1.0\\) accuracy, but the validation accuracy is still above the \\(90\\)% threshold. So, does that mean that this overfit model is still good enough? In truth, we should expect that the validation accuracy to be significantly lesser than the training accuracy. In the next experiment, we’ll vary the noise such that by observation, we can conclude that the validation accuracy will be significantly worse in generalization for an overfit model."
  },
  {
    "objectID": "posts/kernel/index.html#varying-noise",
    "href": "posts/kernel/index.html#varying-noise",
    "title": "Kernel Logistic Regression",
    "section": "Varying Noise",
    "text": "Varying Noise\nIn this experiment, we will repeat the previous experiments with varied noise. How does the choices of noise affect our initial data?\n\nnp.random.seed(15)\n\nX, y = make_moons(200, shuffle = True, noise = 0.001)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nShown above, I’ve fixed the noise to be \\(0.001\\). By observation, choosing some very small noise makes similar feature points closer to each other.\n\nnp.random.seed(16)\n\nX, y = make_moons(200, shuffle = True, noise = 0.1)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(17)\n\nX, y = make_moons(200, shuffle = True, noise = 1)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nIn the two plots above, I’ve increased the noise to be \\(0.1\\) and \\(1\\). It is observed for the crescent-like data to be increasingly nonlinear and dispersed throughout the region. Now, we can experiment how varied noise level affects the training and validation accuracy using kernel logistic regression.\nIn the previous experiment, I’ve fixed noise to be a relatively small value of \\(0.222\\). Let’s choose a lesser and greater noise value of \\(0.05\\) and \\(1\\) and observe what happens to the decision regions when plotted on the trained models. We’ll perform each model twice to record its performance for a smaller and larger noise level with a gamma level of \\(0.1\\).\n\nSmall Noise\n\nnp.random.seed(18)\n\nX, y = make_moons(200, shuffle = True, noise = 0.05)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(19)\nX, y = make_moons(200, shuffle = True, noise = 0.05)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nFor a fitted model whose gamma and noise levels are small, both training and validation accuracy are highly accurate and shows similar performance above the \\(90\\)% accuracy threshold.\n\n\nLarge Noise\n\nnp.random.seed(20)\n\nX, y = make_moons(200, shuffle = True, noise = 1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(21) \n\nX, y = make_moons(200, shuffle = True, noise = 1)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nHowever, for a model with a small gamma and a data with large noise, the model can only do so much to classify the heavily dispersed and nonlinear data. The training accuracy no longer goes at or above the \\(90\\)% threshold we’ve seen before, but considering the large noise level, a small gamma still generates moderate accuracy in classifying the features.\nFor the gamma experiment, we concluded that a smaller gamma value will produce a higher validation accuracy. However, our choice of the gamma actually depends on the choice of noise. A smaller noise will generate a higher validation accuracy than choosing a data with large noise.\n\n\nLarge Gamma and Noise\nIn the gammaexperiment, we tested an overfit model for some data of small noise and a gamma value of \\(10000\\) and observed that its validation accuracy was still above the \\(90\\)% threshold. In this experiment, I will display the results for an overfit model of data with large noise; I’ve chosen the same gamma value and set the data noise to be \\(2\\).\n\nnp.random.seed(22)\n\nX, y = make_moons(200, shuffle = True, noise = 2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(23)\n\nX, y = make_moons(200, shuffle = True, noise = 2)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nShown above, we’ve plotted the decision regions for an overfit model with a gamma value of \\(10000\\) for a data with noise \\(2\\). Though the training accuracy was \\(1.0\\), generalizing the model on unseen test data generated a validation accuracy of \\(0.5\\) which is significantly worse; the model successfully classifies and predicts for unseen test data \\(50\\)% of the time. In contrast to the previous overfit model experiment, we can conclude that choosing a large noise will significantly decrease the validation accuracy below the \\(90\\)% threshold."
  },
  {
    "objectID": "posts/kernel/index.html#other-geometries",
    "href": "posts/kernel/index.html#other-geometries",
    "title": "Kernel Logistic Regression",
    "section": "Other Geometries",
    "text": "Other Geometries\nIn the previous experiments, we’ve generated a moon shaped data. However, will our trained model using kernel logistic regression generate high validation accuracy for a circle-shaped data?\nIn this experiment, I will generate concentric circles instead of crescents using the make_circles function. Shown below are several examples of circle-shaped data with varying noise.\n\nnp.random.seed(24)\n# Small noise\nX, y = make_circles(n_samples= 200, noise=0.001, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(25)\n# Medium noise\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(26)\n# Large noise\nX, y = make_circles(n_samples= 200, noise=1, shuffle = True)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nFor this experiment, we’ll generate a circle-shaped data with small noise and fit our model with small gamma to generate both high testing and validation accuracy. For this experiment, I’ve chosen a noise of \\(0.05\\).\n\nnp.random.seed(27)\n\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Training Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n# Test our trained model on unseen test datta\nX, y = make_circles(n_samples= 200, noise=0.05, shuffle = True)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Validation Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nShown above, we observed that by choosing some small gamma and noise for a circle-shaped data, our trained model generated both high training and validation accuracy above the \\(90\\)% threshold. Thus, this is a pretty successful classifier."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This is my experimentation for a machine learning model onto individual characteristics given a dataset from the American Community Survey’s Public Use Microdata Sample (PUMS). Then, I’ll conduct a fairness audit to assess whether my algorithm possesses bias with respect to demographic characteristics.\n\n\n\n\n\n\nApr 3, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the least-squares linear regression with several experimentations utilizing LASSO regularization.\n\n\n\n\n\n\nMar 27, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the Kernel Logistic Regression using linear empirical risk minimization to learn nonlinear decision boundaries.\n\n\n\n\n\n\nMar 9, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation of gradient descent, a momentum method, and stochastic gradient descent for the optimization for logistic regression.\n\n\n\n\n\n\nMar 5, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is my implementation for the Perceptron Algorithm using numerical programming on synthetic data sets.\n\n\n\n\n\n\nFeb 23, 2023\n\n\nKent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "Welcome to my blog!"
  }
]